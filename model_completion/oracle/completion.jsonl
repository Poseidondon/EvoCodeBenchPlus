{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  inv_scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  x = z / inv_scale\n  return x"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            with SQLiteMemoization(filename) as memoizer:\n                return memoizer.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapped\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f'{cls.__name__}: invalid bbox. x_min={values[\"x_min\"]}, x_max={values[\"x_max\"]},'\n            f' y_min={values[\"y_min\"]}, y_max={values[\"y_max\"]}'\n        )\n\n    return values"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  # Use the fact that ||x - y||^2 == ||x||^2 + ||y||^2 - 2 x^T y.\n  sq_norm0 = np.sum(mat0**2, 0)\n  sq_norm1 = np.sum(mat1**2, 0)\n  sq_dist = sq_norm0[:, None] + sq_norm1[None, :] - 2 * mat0.T @ mat1\n  sq_dist = np.maximum(0, sq_dist)  # Negative values must be numerical errors.\n  return sq_dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n\n    return path.startswith(\"/teamspace/datasets/\") or path.startswith(\"/teamspace/s3_connections/\")"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if dim not in [1, 2]:\n        raise ValueError(f\"dim must be 1 or 2, got {dim}\")\n    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"If `{name}` is provided as a dictionary, you must input `X` as a\"\n                \" DataFrame with assets names in columns\"\n            )\n        if dim == 1:\n            arr = np.array([items.get(asset, fill_value) for asset in assets_names])\n        else:\n            # add assets and convert dict to ordered array\n            arr = {}\n            for asset in assets_names:\n                elem = items.get(asset)\n                if elem is None:\n                    elem = [asset]\n                elif np.isscalar(elem):\n                    elem = [asset, elem]\n                else:\n                    elem = [asset, *elem]\n                arr[asset] = elem\n            arr = (\n                pd.DataFrame.from_dict(arr, orient=\"index\")\n                .loc[assets_names]\n                .to_numpy()\n                .T\n            )\n    else:\n        arr = np.asarray(items)\n\n    if arr.ndim != dim:\n        raise ValueError(f\"`{name}` must be a {dim}D array, got a {arr.ndim}D array\")\n\n    if not isinstance(fill_value, str) and np.isnan(arr).any():\n        raise ValueError(f\"`{name}` contains NaN\")\n\n    if arr.shape[-1] != n_assets:\n        if dim == 1:\n            s = \"(n_assets,)\"\n        else:\n            s = \"(n_groups, n_assets)\"\n        raise ValueError(\n            f\"`{name}` must be a of shape {s} with n_assets={n_assets}, \"\n            f\"got {arr.shape[0]}\"\n        )\n    return arr"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(\n            data[\"dynamic_prompt\"],\n            data[\"purpose\"],\n            data[\"depth\"],\n            agent_lifecycle,\n            openai_wrapper,\n            data[\"max_depth\"],\n            data.get(\"working_agent\", False),\n            data.get(\"is_prime\", False),\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"]\n        )\n\n        if data.get(\"purpose_embedding\") is not None:\n            agent.purpose_embedding = np.array(data[\"purpose_embedding\"])\n        else:\n            agent.purpose_embedding = None\n\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", \"\")\n        return agent"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 25 / 323 * srgb\n  linear1 = xnp.maximum(eps, ((200 * srgb + 11) / (211))) ** (12 / 5)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  n = len(x)\n  spline_degree = min(spline_degree, n - 1)\n\n  if spline_degree > 0:\n    tck = scipy.interpolate.splrep(t_input, x, s=smoothness, k=spline_degree)\n    return scipy.interpolate.splev(t_output, tck).astype(x.dtype)\n  else:  # n = 0 or 1\n    fill_value = x[0] if n else 0.0\n    return np.full(t_output.shape, fill_value, dtype=x.dtype)"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if len(word) < 1 or word.isupper() or word.islower():\n        return word\n    else:\n        # check the first two letters to see if it is just a titled word e.g. Hello\n        if word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            # e.g. return HELLO if HEllo else return hello if heLlo\n            return word.lower() if word[0].islower() else word.upper()"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != np.dtype(\"bool\"):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary. got dtype {v.dtype}\")\n\n    return v"}
{"namespace": "coord.contract3_isoscale", "completion": "  if x.shape[-1] != 3:\n    raise ValueError(f'Inputs must be 3D, are {x.shape[-1]}D.')\n  norm_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1))\n  # Equivalent to cbrt((2 * sqrt(norm_sq) - 1) ** 2) / norm_sq:\n  return jnp.exp(2 / 3 * jnp.log(2 * jnp.sqrt(norm_sq) - 1) - jnp.log(norm_sq))"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if not os.path.exists(summary_path):\n        raise ValueError(f\"summary.csv does not exist in {summary_path}.\")\n    summary_df = pd.read_csv(summary_path)\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    if any([col not in summary_df.columns for col in dict_columns]):\n        raise ValueError(f\"{dict_columns} must be in summary_df.columns.\")\n\n    def convert_dict(elem):\n        return ast.literal_eval(elem)\n\n    summary_df[dict_columns] = summary_df[dict_columns].applymap(convert_dict)\n    return summary_df"}
{"namespace": "coord.isotropize", "completion": "  d = cov.shape[-1]\n  if d == 1:\n    return cov\n  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    diag_val = det ** (1 / d)\n    is_invalid = (det <= jnp.finfo(jnp.float32).tiny) | ~jnp.isfinite(det)\n  elif mode == 'accurate':\n    log_det = jnp.linalg.slogdet(cov)[1]\n    diag_val = jnp.exp(log_det / d)\n    is_invalid = ~jnp.isfinite(log_det)\n  else:\n    raise ValueError(f'mode={mode} not implemented.')\n  cov_iso = jnp.eye(d) * diag_val[Ellipsis, None, None]\n  # Guard against NaN outputs when `det` is super small. Note that this does not\n  # guard against NaN gradients!\n  cov_iso = jnp.where(is_invalid[Ellipsis, None, None], jnp.zeros_like(cov), cov_iso)\n  return cov_iso"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description.\")\n    parser.add_argument(\"--upload-files\", nargs='+', dest=\"upload_files\", help=\"List of files to upload.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task.\")\n    parser.add_argument(\"--record-dir\", type=str, dest=\"record_dir\", help=\"Directory to record task execution logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode: 'auto' or 'manual'.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", default=False, help=\"Run in quiet mode; minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, dest=\"max_subtask_chain_length\",\n                        help=\"Maximum length of subtask chain.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", dest=\"enable_ask_human_for_help\",\n                        help=\"Flag to enable asking for human assistance.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, dest=\"max_plan_refine_chain_length\",\n                        help=\"Maximum length of plan refinement chain.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, dest=\"max_plan_tree_depth\",\n                        help=\"Maximum depth of the plan tree.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, dest=\"max_plan_tree_width\",\n                        help=\"Maximum width of the plan tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, dest=\"max_retry_times\", help=\"Maximum number of retry attempts.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'),\n                        dest=\"config_file\", help=\"Path to the configuration file.\")\n\n    return parser.parse_args()"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if len(v.shape) != 2 or v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must have shape (_, 2).\")\n\n    return v"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[-1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'hash':\n    fn = hash_resample.hash_resample_3d\n  elif datastructure == 'grid':\n    # Note: unlike hash_resample_3d, resample_3d expects integer coordinate\n    # voxel centers, so we offset the coordinates by 0.5 here. We also\n    # flip the input coordinates since the convention used in `resample_3d`\n    # is for input point (x, y, z) to index grid_values[z, y, x]. We prefer the\n    # grid axis order to align with the Cartesian coordinate axes.\n    coordinates = jnp.flip(coordinates - 0.5, axis=-1)\n\n    def fn(v, c):\n      \"\"\"Add and remove two extra dims at the front of coord/output tensors.\"\"\"\n      return resample.resample_3d(v, c[None, None])[0, 0]\n\n  else:\n    raise ValueError(\n        'datastructure must be either `grid` or `hash` but '\n        f'`{datastructure}` was given.'\n    )\n\n  coordinates_flat = coordinates.reshape(-1, coordinates.shape[-1])\n  if values.dtype != coordinates_flat.dtype:\n    coordinates_flat = coordinates_flat.astype(values.dtype)\n  result_flat = fn(values, coordinates_flat)\n  result = result_flat.reshape(coordinates.shape[:-1] + (values.shape[-1],))\n  return result"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  int_weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      int_weights.append((i, j, v - (i + j)))\n  int_weights = np.array(int_weights)\n  weights = int_weights / v  # Barycentric weights.\n  return weights"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  interp = functools.partial(jnp.interp, left=0, right=0)\n  return jnp.vectorize(interp, signature='(n),(m),(m)->(n)')(tq, t, v)"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not np.array([value >= 0 for value in v]).all():\n            raise ValueError(f\"{cls.__name__}: all {field.name} must be positive. Received {v}\")\n    elif v < 0.0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive. Received {v}\")\n\n    return v"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  t = -(near + origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  dx, dy, dz = xnp.moveaxis(directions, -1, 0)\n  ox, oy, oz = xnp.moveaxis(origins, -1, 0)\n\n  xmult = 1.0 / pixtocam[0, 2]  # Equal to -2. * focal / cx\n  ymult = 1.0 / pixtocam[1, 2]  # Equal to -2. * focal / cy\n\n  # Perspective projection into NDC for the t = 0 near points\n  #     origins + 0 * directions\n  origins_ndc = xnp.stack(\n      [xmult * ox / oz, ymult * oy / oz, -xnp.ones_like(oz)], axis=-1\n  )\n\n  # Perspective projection into NDC for the t = infinity far points\n  #     origins + infinity * directions\n  infinity_ndc = xnp.stack(\n      [xmult * dx / dz, ymult * dy / dz, xnp.ones_like(oz)], axis=-1\n  )\n\n  # directions_ndc points from origins_ndc to infinity_ndc\n  directions_ndc = infinity_ndc - origins_ndc\n\n  return origins_ndc, directions_ndc"}
{"namespace": "geometry.are_lines_parallel", "completion": "  eps = jnp.finfo(jnp.float32).eps\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.dot(dir1, dir2) >= 1.0 - eps  # pytype: disable=bad-return-type  # jnp-type"}
{"namespace": "common.bleu4_score", "completion": "\n    import math\n    from nltk.translate.bleu_score import sentence_bleu\n\n    # Tokenize the continuation and reference texts using the custom tokenizer function\n    continuation_tokens = custom_tokenizer(continuation)\n    reference_tokens = custom_tokenizer(reference)\n\n    # Calculate the BLEU score using the nltk.translate.bleu_score.sentence_bleu function"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(w, td))"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    for dirpath, _, filenames in os.walk(str(path)):\n        for filename in filenames:\n            with contextlib.suppress(FileNotFoundError):\n                size += os.stat(os.path.join(dirpath, filename)).st_size\n    return size"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        purpose_embedding = agent.purpose_embedding\n        if isinstance(purpose_embedding, np.ndarray):\n            purpose_embedding = purpose_embedding.tolist()  # Convert ndarray to list\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(f\"Items and weights must have the same length, got {len(items)} and {len(weights)}.\")\n    if any(w <= 0 for w in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    sorted_items_and_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n    bin_contents = defaultdict(list)\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    for item, weight in sorted_items_and_weights:\n        min_bin_id = min(bin_weights, key=(lambda x: bin_weights[x]), default=0)\n        bin_contents[min_bin_id].append(item)\n        bin_weights[min_bin_id] += weight\n\n    return bin_contents, bin_weights"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = f\"{func_name}:{repr(args)}:{repr(kwargs)}\".encode(\"utf-8\")\n        return hashlib.sha256(data).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if polygon.ndim != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {polygon.shape}\")\n\n    inter_point_distances = np.linalg.norm(np.roll(polygon, 1, axis=0) - polygon, axis=1)\n    inter_point_distances = inter_point_distances[inter_point_distances < max_point_distance]\n\n    return inter_point_distances.sum()"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [area(polygon) if len(polygon) > 2 else 1.0 for polygon in polygons]\n    area_factors = np.array(areas) / np.max(areas)\n\n    filtered_polygons = [\n        polygon\n        for area, area_factor, polygon in zip(areas, area_factors, polygons)\n        if area > abs_tr and area_factor > rel_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    divisible_num_batches_yielded = num_samples_yielded // (num_workers * batch_size)\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = divisible_num_batches_yielded * batch_size\n\n    num_samples_yielded = num_samples_yielded - (num_workers * divisible_num_batches_yielded * batch_size)\n\n    # take care of the reminder\n    worker_idx = 0  # reset the worker_idx\n    while True:\n        if num_samples_yielded >= batch_size:\n            indexes[worker_idx] += batch_size\n            worker_idx = (worker_idx + 1) % num_workers\n            num_samples_yielded -= batch_size\n        else:\n            indexes[worker_idx] += num_samples_yielded\n            break\n    return indexes"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value), \"results and value must have the same length.\"\n    try:\n        filtered_results, _, filtered_metadatas = zip(\n            *filter(lambda x: x[1] <= threshold, zip(results, value, metadatas)))\n    except ValueError:\n        return [], []\n    return list(filtered_results), list(filtered_metadatas)"}
{"namespace": "iris.utils.math.area", "completion": "    if len(array.shape) != 2 or array.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the area of a polygon with shape {array.shape}. Expecting (_, 2).\")\n\n    xs, ys = array.T\n    area = 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))\n\n    return float(area)"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    i = torch.arange(a.shape[-1], device=a.device)  # 128\n    v_ge_a = v[..., None, :] >= a[..., :, None]\n    idx_lo = torch.max(torch.where(v_ge_a, i[..., :, None], i[..., :1, None]), -2)[0]  # 128\n    idx_hi = torch.min(torch.where(~v_ge_a, i[..., :, None], i[..., -1:, None]), -2)[0]\n    return idx_lo, idx_hi"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1.0],\n  ])"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if the array has the right number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions and (v.shape != (0,) or nb_dimensions != 0):\n            raise ValueError(\n                f\"{cls.__name__}: wrong number of dimensions for {field.name}. \"\n                f\"Expected {nb_dimensions}, got {len(v.shape)}\"\n            )\n\n        return v\n\n    return validator"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x = cartesian_vector[Ellipsis, 0]\n  y = cartesian_vector[Ellipsis, 1]\n  z = cartesian_vector[Ellipsis, 2]\n\n  r = optax.safe_norm(cartesian_vector, min_norm=eps, axis=-1)\n  theta = spin_math.safe_acos(z / r)\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "common.rougeL_score", "completion": "\n        # Calculate the brevity penalty factor\n        if continuation_length > reference_length:\n            brevity_penalty = 1\n        else:"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    obj = pydoc.locate(name)\n\n    # Some cases (e.g. torch.optim.sgd.SGD) not handled correctly\n    # by pydoc.locate. Try a private function from hydra.\n    if obj is None:\n        try:\n            # from hydra.utils import get_method - will print many errors\n            from hydra.utils import _locate\n        except ImportError as e:\n            raise ImportError(f\"Cannot dynamically locate object {name}!\") from e\n        else:\n            obj = _locate(name)  # it raises if fails\n\n    return obj"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        xn = x * 100\n        f = \"%\"\n    else:\n        xn = x\n        f = \"f\"\n    if xn == 0:\n        n = 0\n    else:\n        n = min(6, max(int(-np.log10(abs(xn))) + 2, 2))\n    return \"{value:{fmt}}\".format(value=x, fmt=f\".{n}{f}\")"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    usage = shutil.disk_usage(input_dir)\n\n    while (usage.free / 1000 / 1000 / 1000) <= threshold_in_gb:\n        sleep(sleep_time)\n        usage = shutil.disk_usage(input_dir)\n\n    return"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  return p * jnp.diff(t)"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = re.sub(r\"\\s+\", \"\", line_text)\n    return su.segment(line_text)"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    k = np.random.rand(n)\n    if zeros > 0:\n        zeros_idx = np.random.choice(n, zeros, replace=False)\n        k[zeros_idx] = 0\n    return k / sum(k)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        _module_dict = deepcopy(module_dict)\n        module_type = _module_dict.pop('module_type')\n        module_params = _module_dict\n        return cls(module_type, module_params)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_size = np.asarray(crop_size, dtype=np.int32)\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    center_yx = (bbox[1] + bbox[3]) * 0.5, (bbox[0] + bbox[2]) * 0.5\n    assert (\n        image_size[0] >= center_yx[0] and image_size[1] >= center_yx[1]\n    ), \"The annotation bounding box is outside of the image!\"\n    assert (\n        image_size[0] >= crop_size[0] and image_size[1] >= crop_size[1]\n    ), \"Crop size is larger than image size!\"\n\n    min_yx = np.maximum(np.floor(center_yx).astype(np.int32) - crop_size, 0)\n    max_yx = np.maximum(np.asarray(image_size, dtype=np.int32) - crop_size, 0)\n    max_yx = np.minimum(max_yx, np.ceil(center_yx).astype(np.int32))\n\n    y0 = np.random.randint(min_yx[0], max_yx[0] + 1)\n    x0 = np.random.randint(min_yx[1], max_yx[1] + 1)\n    return T.CropTransform(x0, y0, crop_size[1], crop_size[0])"}
{"namespace": "ref_utils.l2_normalize", "completion": "  tiny = jnp.finfo(jnp.float32).tiny\n  grad_eps = jnp.maximum(tiny, grad_eps)\n  denom_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  normal_val = x / jnp.sqrt(jnp.maximum(tiny, denom_sq))\n  normal_grad = x / jnp.sqrt(jnp.maximum(grad_eps, denom_sq))\n  # Use `normal_val` in the forward pass but `normal_grad` in the backward pass.\n  normal = math_lib.override_gradient(normal_val, normal_grad)\n  return jnp.where(denom_sq < tiny, jnp.zeros_like(normal), normal)"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        split_info = agent_info.split(\":\", 1)\n        agent_name = split_info[0].strip()\n        input_text = split_info[1].strip() if len(split_info) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = (\n        np.stack(\n            [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n        )\n        if len(annos)\n        else np.zeros((0, 4))\n    )\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n\n    classes = [int(obj[\"category_id\"]) for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            try:\n                masks = PolygonMasks(segms)\n            except ValueError as e:\n                raise ValueError(\n                    \"Failed to use mask_format=='polygon' from the given annotations!\"\n                ) from e\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expect segmentation of 2 dimensions, got {}.\".format(\n                        segm.ndim\n                    )\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                        \" in a 2D numpy array of shape HxW.\".format(type(segm))\n                    )\n            # torch.from_numpy does not support array with negative stride.\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    classes = {type(x) for x in model.modules()}\n    # __constants__ is the old way to annotate constants and not compatible\n    # with __annotations__ .\n    classes = {x for x in classes if not hasattr(x, \"__constants__\")}\n    for cls in classes:\n        cls.__annotations__[\"training\"] = torch.jit.Final[bool]\n    yield\n    for cls in classes:\n        cls.__annotations__[\"training\"] = bool"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metrics_copy = deepcopy(metrics)\n    if not isinstance(metrics_copy, list):\n        raise ValueError(\"metrics must be a list of string or dictionary.\")\n    if isinstance(metrics_copy[0], str):\n        return metrics_copy, [{} for _ in metrics_copy]\n    elif isinstance(metrics_copy[0], dict):\n        # pop 'metric_name' key from dictionary\n        metric_names = list(map(lambda x: x.pop('metric_name'), metrics_copy))\n        metric_params = [dict(map(lambda x, y: cast_embedding_model(x, y), metric.keys(), metric.values())) for metric\n                         in metrics_copy]\n        return metric_names, metric_params\n    else:\n        raise ValueError(\"metrics must be a list of string or dictionary.\")"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn is None:\n    fn_fwd = lambda x: x\n    fn_inv = lambda x: x\n  else:\n    fn_fwd = fn\n    if fn_inv is None:\n      # A simple mapping from some functions to their inverse.\n      inv_mapping = {\n          'reciprocal': jnp.reciprocal,\n          'log': jnp.exp,\n          'exp': jnp.log,\n          'sqrt': jnp.square,\n          'square': jnp.sqrt,\n      }\n      fn_inv = inv_mapping[fn.__name__]\n  fn_t_near, fn_t_far = [fn_fwd(t) for t in (t_near, t_far)]\n  # Forcibly clip t to the range of valid values, to guard against inf's.\n  t_clip = lambda t: jnp.clip(t, t_near, t_far)\n  t_to_s = lambda t: (fn_fwd(t_clip(t)) - fn_t_near) / (fn_t_far - fn_t_near)\n  s_to_t = lambda s: t_clip(fn_inv(s * fn_t_far + (1 - s) * fn_t_near))\n  return t_to_s, s_to_t"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.stack([x, y, z], axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return 0.5 * jnp.sum((w[Ellipsis, :-1] + w[Ellipsis, 1:]) * jnp.diff(t), axis=-1)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    df = pd.concat([pd.Series(dict(zip(_id, score))) for _id, score in zip(ids, scores)], axis=1)\n    normalized_scores = (df - df.min()) / (df.max() - df.min())\n    normalized_scores = normalized_scores.fillna(0)\n    normalized_scores['weighted_sum'] = normalized_scores.mul(weights).sum(axis=1)\n    normalized_scores = normalized_scores.sort_values(by='weighted_sum', ascending=False)\n    return normalized_scores.index.tolist()[:top_k], normalized_scores['weighted_sum'][:top_k].tolist()"}
{"namespace": "coord.track_linearize", "completion": "  if (len(mean.shape) + 1) != len(cov.shape):\n    raise ValueError('cov must be non-diagonal')\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n  fn_cov = jax.vmap(lin_fn, -1, -2)(jax.vmap(lin_fn, -1, -2)(cov))\n  return fn_mean, fn_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for e in x:\n        n = len(e)\n        if n > 1:\n            mid = n // 2\n            yield [e[0:mid], e[mid:n]]"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = x[Ellipsis, None, :] * scales[:, None]  # (..., s, c).\n  scaled_x = jnp.reshape(scaled_x, shape)  # (..., s*c).\n  # Note that we're not using safe_sin, unlike IPE.\n  # (..., s*c + s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_x, scaled_x + 0.5 * jnp.pi], axis=-1)\n  )\n  if append_identity:\n    return jnp.concatenate([x, four_feat], axis=-1)\n  else:\n    return four_feat"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        shapes_field_1 = [element.shape for element in values[field1]]\n        shapes_field_2 = [element.shape for element in values[field2]]\n\n        if len(values[field1]) != len(values[field2]) or shapes_field_1 != shapes_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shapes_field_1} and {shapes_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.W, camera.H)\n        self.render(camera)"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        n_positions=bert_config.max_position_embeddings,  # No absolute position embedding\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        embd_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        bos_token_id=None,  # TODO: check this\n        eos_token_id=None,\n        # These are new arguments not in the original GPT2Config\n        prenorm=False,\n        parallel_block=False,\n        parallel_block_tied_norm=False,\n        rotary_emb_fraction=getattr(bert_config, \"rotary_emb_fraction\", 0),\n        tie_word_embeddings=True,\n        fused_dropout_add_ln=True,\n        fused_bias_fc=True,\n        use_flash_attn=True,\n        use_xentropy=True,\n        qkv_proj_bias=getattr(bert_config, \"qkv_proj_bias\", True),\n        rotary_emb_base=getattr(bert_config, \"rotary_emb_base\", 1000),\n        rotary_emb_scale_base=getattr(bert_config, \"rotary_emb_scale_base\", None),\n        rotary_emb_interleaved=getattr(bert_config, \"rotary_emb_interleaved\", False),\n        mlp_fc1_bias=getattr(bert_config, \"mlp_fc1_bias\", True),\n        mlp_fc2_bias=getattr(bert_config, \"mlp_fc2_bias\", True),\n        use_rms_norm=getattr(bert_config, \"use_rms_norm\", False),\n        causal=False,\n        type_vocab_size=bert_config.type_vocab_size,\n        dense_seq_output=True,\n        pad_vocab_size_multiple=getattr(bert_config, \"pad_vocab_to_multiple_of\", 1),\n        rotary_scaling_factor=getattr(bert_config, \"rotary_scaling_factor\", None),\n    )"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # For point rendering\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glUseProgram(self.point_program)\n            self.use_gl_program(self.point_program)\n        else:\n            gl.glUseProgram(self.mesh_program)\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()  # slow sync and copy operation # MARK: SYNC\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr[y:h, x:w])  # to gpu, might slow down?"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert len(camera_matrix.size()) == 3, \"This function requires batched inputs!\"\n    assert len(R.size()) == 3, \"This function requires batched inputs!\"\n    assert len(tvec.size()) in (2, 3), \"This function reuqires batched inputs!\"\n\n    # Validate parameters.\n    image_size_wh = image_size.to(R).flip(dims=(1,))\n    assert torch.all(\n        image_size_wh > 0\n    ), \"height and width must be positive but min is: %s\" % (\n        str(image_size_wh.min().item())\n    )\n    assert (\n        camera_matrix.size(1) == 3 and camera_matrix.size(2) == 3\n    ), \"Incorrect camera matrix shape: expected 3x3 but got %dx%d\" % (\n        camera_matrix.size(1),\n        camera_matrix.size(2),\n    )\n    assert (\n        R.size(1) == 3 and R.size(2) == 3\n    ), \"Incorrect R shape: expected 3x3 but got %dx%d\" % (\n        R.size(1),\n        R.size(2),\n    )\n    if len(tvec.size()) == 2:\n        tvec = tvec.unsqueeze(2)\n    assert (\n        tvec.size(1) == 3 and tvec.size(2) == 1\n    ), \"Incorrect tvec shape: expected 3x1 but got %dx%d\" % (\n        tvec.size(1),\n        tvec.size(2),\n    )\n    # Check batch size.\n    batch_size = camera_matrix.size(0)\n    assert R.size(0) == batch_size, \"Expected R to have batch size %d. Has size %d.\" % (\n        batch_size,\n        R.size(0),\n    )\n    assert (\n        tvec.size(0) == batch_size\n    ), \"Expected tvec to have batch size %d. Has size %d.\" % (\n        batch_size,\n        tvec.size(0),\n    )\n    # Check image sizes.\n    image_w = image_size_wh[0, 0]\n    image_h = image_size_wh[0, 1]\n    assert torch.all(\n        image_size_wh[:, 0] == image_w\n    ), \"All images in a batch must have the same width!\"\n    assert torch.all(\n        image_size_wh[:, 1] == image_h\n    ), \"All images in a batch must have the same height!\"\n    # Focal length.\n    fx = camera_matrix[:, 0, 0].unsqueeze(1)\n    fy = camera_matrix[:, 1, 1].unsqueeze(1)\n    # Check that we introduce less than 1% error by averaging the focal lengths.\n    fx_y = fx / fy\n    if torch.any(fx_y > 1.01) or torch.any(fx_y < 0.99):\n        warn_once_about_pulsar_fxfy()\n    f = (fx + fy) / 2\n    # Normalize f into normalized device coordinates.\n    focal_length_px = f / image_w\n    # Transfer into focal_length and sensor_width.\n    # NOTE: Using torch.tensor instead of torch.as_tensor will cause cpu gpu sync\n    focal_length = torch.as_tensor([znear - 1e-5], dtype=torch.float32, device=R.device)\n    focal_length = focal_length[None, :].repeat(batch_size, 1)\n    sensor_width = focal_length / focal_length_px\n    # Principal point.\n    cx = camera_matrix[:, 0, 2].unsqueeze(1)\n    cy = camera_matrix[:, 1, 2].unsqueeze(1)\n    # Transfer principal point offset into centered offset.\n    cx = -(cx - image_w / 2)\n    cy = cy - image_h / 2\n    # Concatenate to final vector.\n    param = torch.cat([focal_length, sensor_width, cx, cy], dim=1)\n    R_trans = R.permute(0, 2, 1)\n    cam_pos = -torch.bmm(R_trans, tvec).squeeze(2)\n    cam_rot = matrix_to_rotation_6d(R_trans)\n    cam_params = torch.cat([cam_pos, cam_rot, param], dim=1)\n    return cam_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    C = -batch.R.mT @ batch.T  # B, 3, 1\n    R = batch.R.clone()\n    R[..., 0, :] *= -1  # flip x row\n    R[..., 1, :] *= -1  # flip y row\n    T = (-R @ C)[..., 0]  # c2w back to w2c\n    R = R.mT  # applied left (left multiply to right multiply, god knows why...)\n\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = get_pytorch3d_ndc_K(batch.K, H, W)\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        old = gl.glGetInteger(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # write buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, x + w, y + h,  # the height is flipped\n                             x, y, x + w, y + h,  # the height is flipped\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    cy1 = torch.cat([torch.zeros_like(y1[..., :1]), torch.cumsum(y1, dim=-1)], dim=-1)  # 129\n    idx_lo, idx_hi = searchsorted(t1, t0)\n\n    cy1_lo = torch.take_along_dim(cy1, idx_lo, dim=-1)  # 128\n    cy1_hi = torch.take_along_dim(cy1, idx_hi, dim=-1)\n\n    y0_outer = cy1_hi[..., 1:] - cy1_lo[..., :-1]  # 127\n    y0_inner = torch.where(idx_hi[..., :-1] <= idx_lo[..., 1:], cy1_lo[..., 1:] - cy1_hi[..., :-1], 0)\n    return y0_inner, y0_outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n    \"\"\"The proposal weight should be an upper envelope on the nerf weight.\"\"\"\n    _, w_outer = inner_outer(t, t_env, w_env)\n    # We assume w_inner <= w <= w_outer. We don't penalize w_inner because it's\n    # more effective to pull w_outer up than it is to push w_inner down.\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    return (w - w_outer).clip(0.).pow(2) / (w + eps)"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"Compute iint w[i] w[j] |t[i] - t[j]| di dj.\"\"\"\n    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    return loss_inter + loss_intra"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    # We want to interpolate into the integrated weights according to `ps`.\n    # Vmap fn to an arbitrary number of leading dimensions.\n    cw_mat = cw.reshape([-1, cw.shape[-1]])\n    t_mat = t.reshape([-1, t.shape[-1]])\n    wprctile_mat = interpolate(torch.as_tensor(ps).to(t, non_blocking=True),\n                               cw_mat,\n                               t_mat)\n    wprctile = wprctile_mat.reshape(cw.shape[:-1] + (len(ps),))\n    return wprctile"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    if t.ndim == w.ndim + 1:\n        t = t[..., 0]  # remove last dim\n\n    # preparing for size change\n    sh = *t.shape[:-1], num_samples  # B, P, I\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # eps = torch.finfo(torch.float32).eps\n    eps = 1e-8\n\n    # Draw uniform samples.\n\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps if perturb else 0\n    d = 1 if single_jitter else num_samples\n    u = (\n        torch.linspace(0, 1 - u_max, num_samples, device=t.device, dtype=t.dtype) +\n        torch.rand(t.shape[:-1] + (d,), device=t.device, dtype=t.dtype) * max_jitter\n    )\n\n    u = invert_cdf(u, t, w)\n\n    # preparing for size change\n    u = u.reshape(sh)\n    return u"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"Dilate (via max-pooling) a non-negative step function.\"\"\"\n    t0 = t[..., :-1] - dilation\n    t1 = t[..., 1:] + dilation\n    t_dilate = torch.sort(torch.cat([t, t0, t1], dim=-1), dim=-1)[0]\n    t_dilate = t_dilate.clip(*domain)\n    w_dilate = torch.max(\n        torch.where(\n            (t0[..., None, :] <= t_dilate[..., None])\n            & (t1[..., None, :] > t_dilate[..., None]),\n            w[..., None, :],\n            0,\n        ),\n        dim=-1)[0][..., :-1]\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    idx_lo, idx_hi = searchsorted(t, tq)\n    yq = torch.where(idx_lo == idx_hi, outside_value,\n                     torch.take_along_dim(torch.cat([y, torch.full_like(y[..., :1], outside_value)], dim=-1), idx_lo, dim=-1))  # ?\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Optionally anneal the weights as a function of training iteration.\n    if anneal_slope > 0:\n        # Schlick's bias function, see https://arxiv.org/abs/2010.09714\n        def bias(x, s): return (s * x) / ((s - 1) * x + 1)\n        anneal = bias(train_frac, anneal_slope)\n    else:\n        anneal = 1.\n\n    # A slightly more stable way to compute weights**anneal. If the distance\n    # between adjacent intervals is zero then its weight is fixed to 0.\n    logits_resample = torch.where(\n        t[..., 1:] > t[..., :-1],\n        anneal * torch.log(w.clip(eps)), -torch.inf)  # MARK: prone to nan\n\n    # If all samples are -inf, softmax will produce a nan (all -torch.inf)\n    w = torch.softmax(logits_resample, dim=-1)\n    return w"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: (to_cuda(v, device, ignore_list) if k != \"meta\" else v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    return batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if v.ndim == (f.ndim + 1): f = f[None].expand(v.shape[0], *f.shape)\n    # assert verts.shape[0] == faces.shape[0]\n    shape = torch.tensor(v.shape)\n    remainder = shape.flip(0)[:(len(shape) - dim - 1) % len(shape)]\n    return multi_gather(v, f.view(*f.shape[:-2], -1), dim=dim).view(*f.shape, *remainder)  # B, F, 3, 3"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        meta = dotdict()\n        meta.H = torch.as_tensor(self.H)\n        meta.W = torch.as_tensor(self.W)\n        meta.K = torch.as_tensor(self.K.to_list(), dtype=torch.float).mT\n        meta.R = torch.as_tensor(self.R.to_list(), dtype=torch.float).mT\n        meta.T = torch.as_tensor(self.T.to_list(), dtype=torch.float)[..., None]\n        meta.n = torch.as_tensor(self.n, dtype=torch.float)\n        meta.f = torch.as_tensor(self.f, dtype=torch.float)\n        meta.t = torch.as_tensor(self.t, dtype=torch.float)\n        meta.v = torch.as_tensor(self.v, dtype=torch.float)\n        meta.bounds = torch.as_tensor(self.bounds.to_list(), dtype=torch.float)  # no transpose for bounds\n\n        # GUI related elements\n        meta.mass = torch.as_tensor(self.mass, dtype=torch.float)\n        meta.moment_of_inertia = torch.as_tensor(self.moment_of_inertia, dtype=torch.float)\n        meta.movement_force = torch.as_tensor(self.movement_force, dtype=torch.float)\n        meta.movement_torque = torch.as_tensor(self.movement_torque, dtype=torch.float)\n        meta.movement_speed = torch.as_tensor(self.movement_speed, dtype=torch.float)\n        meta.origin = torch.as_tensor(self.origin.to_list(), dtype=torch.float)\n        meta.world_up = torch.as_tensor(self.world_up.to_list(), dtype=torch.float)\n\n        batch = dotdict()\n        batch.update(meta)\n        batch.meta.update(meta)\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "\n        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent: Optional[Agent] = None\n        highest_similarity: float = -np.inf\n\n        try:\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                   agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n\n                if similarity > highest_similarity:\n                    highest_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, highest_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            PRIME_PROMPT, PRIME_NAME, 0, self, \n            self.openai_wrapper, PRIME_AGENT_WEIGHT, True, True\n        )\n        self.agents.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        Load an agent with the given purpose from the database.\n        \"\"\"\n        serialized_agent = self.persistence.fetch_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        Load all agents from the database.\n        \"\"\"\n        purposes = self.persistence.load_all_purposes()\n        agents = []\n        for purpose in purposes:\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        messages = [\n            {\"role\": \"system\", \"content\": PROMPT_ENGINEERING_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)}\n        ]\n\n        try:\n            return self.openai_wrapper.chat_completion(messages=messages)\n        except Exception as e:\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\n                # add id field\n                \"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict))\n            )"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            return json.loads(row[0]) if row else None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        return json.loads(row[0]) if row else None"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result))\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    args_dict = vars(args)\n    for key, value in args_dict.items():\n        if value is not None:\n            if key == 'model':\n                ARGS['default_completion_kwargs'] = deepcopy(CONFIG['default_completion_kwargs'])\n                ARGS['default_completion_kwargs']['model'] = value\n            else:\n                ARGS[key] = value\n\n    # Redirect stdout to a file if quiet mode is true\n    if quiet_mode:\n        from XAgent.running_recorder import recorder\n        record_file_path = os.path.join(recorder.record_root_dir, \"command_line.ansi\")\n        with open(record_file_path, \"w\", encoding=\"utf-8\") as file, redirect_stdout(file):\n            start_command_line(args_dict)\n    else:\n        start_command_line(args_dict)"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n\n        request_timeout = kwargs.pop(\"request_timeout\", 60)\n        if \"api_version\" in chatcompletion_kwargs:\n            if \"base_url\" in chatcompletion_kwargs:\n                base_url = chatcompletion_kwargs.pop(\"base_url\", None)\n            else:\n                base_url = chatcompletion_kwargs.pop(\"api_base\", None)\n            azure_endpoint = chatcompletion_kwargs.pop(\"azure_endpoint\", base_url)\n            api_version = chatcompletion_kwargs.pop(\"api_version\", None)\n            api_key = chatcompletion_kwargs.pop(\"api_key\", None)\n            chatcompletion_kwargs.pop(\"api_type\", None)\n            if \"engine\" in chatcompletion_kwargs:\n                model = chatcompletion_kwargs.pop(\"engine\", None)\n            else:\n                model = chatcompletion_kwargs.pop(\"model\", None)\n            chatcompletion_kwargs.update({\"model\": model})\n            chatcompletion_kwargs.update(kwargs)\n            client = openai.AzureOpenAI(\n                api_key=api_key,\n                azure_endpoint=azure_endpoint,\n                api_version=api_version,\n                timeout=request_timeout,\n            )\n        else:\n            if \"base_url\" in chatcompletion_kwargs:\n                base_url = chatcompletion_kwargs.pop(\"base_url\", None)\n            else:\n                base_url = chatcompletion_kwargs.pop(\"api_base\", None)\n            api_key = chatcompletion_kwargs.pop(\"api_key\", None)\n            organization = chatcompletion_kwargs.pop(\"organization\", None)\n            chatcompletion_kwargs.update(kwargs)\n            client = openai.OpenAI(\n                api_key=api_key,\n                organization=organization,\n                base_url=base_url,\n                timeout=request_timeout\n            )\n        try:\n            completions = client.chat.completions.create(**chatcompletion_kwargs)\n            response = completions.model_dump()\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\n                    message=\"maximum context length exceeded\", response=None, body=None\n                )\n\n        except BadRequestError as e:\n            if \"maximum context length\" in e.message:\n                if model_name == \"gpt-4\" and \"gpt-4-32k\" in CONFIG.api_keys:\n                    model_name = \"gpt-4-32k\"\n                elif model_name == \"gpt-4\" and \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                    model_name = \"gpt-4-1106-preview\"\n                else:\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n\n                print(f\"max context length reached, retrying with {model_name}\")\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                request_timeout = kwargs.pop(\"request_timeout\", 60)\n                if \"base_url\" in chatcompletion_kwargs:\n                    base_url = chatcompletion_kwargs.pop(\"base_url\", None)\n                else:\n                    base_url = chatcompletion_kwargs.pop(\"api_base\", None)\n                api_key = chatcompletion_kwargs.pop(\"api_key\", None)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n                completions = client.chat.completions.create(**chatcompletion_kwargs)\n                response = completions.model_dump()\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None:\n            self._create_client()\n            self._last_time = time()\n\n        # Re-generate credentials for EC2\n        if self._last_time is None or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            self._state_dict[\"num_samples_yielded\"] = num_samples_yielded\n            return self._state_dict\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if state_dict:\n            # the state is restored within the workers\n            self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n                )\n        elif state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                \"The provided `input_dir` URL state doesn't match the current one. \"\n                f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                \"The provided `seed` state doesn't match the current one. \"\n                f\"Found `{self.seed}` instead of `{state['seed']}`.\"\n            )\n\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                \"The provided `item_loader` state doesn't match the current one. \"\n                f\"Found `{self.item_loader.state_dict()}` instead of `{state['item_loader']}`.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                \"The provided `drop_last` state doesn't match the current one. \"\n                f\"Found `{self.drop_last}` instead of `{state['drop_last']}`.\"\n            )"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    hash_object = hashlib.md5((input_dir or \"\").encode())\n    if \"LIGHTNING_CLUSTER_ID\" not in os.environ or \"LIGHTNING_CLOUD_PROJECT_ID\" not in os.environ:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    cache_dir = os.path.join(\"/cache\", \"chunks\", hash_object.hexdigest())\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        obj = parse.urlparse(remote_filepath)\n\n        if obj.scheme != \"s3\":\n            raise ValueError(f\"Expected obj.scheme to be `s3`, instead, got {obj.scheme} for remote={remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        try:\n            with FileLock(local_filepath + \".lock\", timeout=3 if obj.path.endswith(_INDEX_FILENAME) else 0):\n                if self._s5cmd_available:\n                    proc = subprocess.Popen(\n                        f\"s5cmd cp {remote_filepath} {local_filepath}\",\n                        shell=True,\n                        stdout=subprocess.PIPE,\n                    )\n                    proc.wait()\n                else:\n                    from boto3.s3.transfer import TransferConfig\n\n                    extra_args: Dict[str, Any] = {}\n\n                    # try:\n                    #     with FileLock(local_filepath + \".lock\", timeout=1):\n                    if not os.path.exists(local_filepath):\n                        # Issue: https://github.com/boto/boto3/issues/3113\n                        self._client.client.download_file(\n                            obj.netloc,\n                            obj.path.lstrip(\"/\"),\n                            local_filepath,\n                            ExtraArgs=extra_args,\n                            Config=TransferConfig(use_threads=False),\n                        )\n        except Timeout:\n            # another process is responsible to download that file, continue\n            pass"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_idx:\n                continue\n\n            worker_chunks.append(chunk_index)\n            worker_intervals.append(chunk_interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode.encode(\"utf-8\")\n        width, height = item.size\n        raw = item.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode(\"utf-8\")\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            return {\n                \"dataset\": self.dataset.state_dict(\n                    self._num_samples_yielded_streaming, self.num_workers, self.batch_size\n                ),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        num_samples_yieled = [0 for _ in range(len(list(self._num_samples_yielded_combined.values())[0]))]\n        for worker_idx in self._num_samples_yielded_combined:\n            for dataset_idx, samples_yieled in enumerate(self._num_samples_yielded_combined[worker_idx]):\n                num_samples_yieled[dataset_idx] += samples_yieled\n\n        return {\n            \"dataset\": self.dataset.state_dict(self.num_workers, self.batch_size, num_samples_yieled),\n            \"current_epoch\": self.current_epoch if self.restore else self.current_epoch - 1,\n            \"latest_worker_idx\": self._latest_worker_idx,\n            \"num_samples_yielded\": deepcopy(self._num_samples_yielded_combined),\n        }"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"av is required. Run `pip install av`\")\n\n        # Add support for a better deserialization mechanism for videos\n        # TODO: Investigate https://pytorch.org/audio/main/generated/torchaudio.io.StreamReader.html\n        import torchvision.io\n\n        with tempfile.TemporaryDirectory() as dirname:\n            fname = os.path.join(dirname, \"file.mp4\")\n            with open(fname, \"wb\") as stream:\n                stream.write(data)\n            return torchvision.io.read_video(fname, pts_unit=\"sec\")"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        filepaths: List[str] = []\n        if self.filled:\n            return filepaths\n\n        # Try writing down an chunks\n        while self._should_write():\n            filepaths.append(self.write_chunk())\n\n        # If any elements is left, try writing one last chunk\n        if self._serialized_items:\n            filepaths.append(self.write_chunk(True))\n\n        # Write down the index file\n        self.write_chunks_index()\n\n        self._is_done = True\n        return filepaths"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        else:\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n\n        # Used to restart on the next DataLoader worker from the previous run.\n        self._latest_worker_idx = obj[\"latest_worker_idx\"] + 1\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)\n\n        # Inform we are resuming and disable resetting the StreamingDataLoader state.\n        # This is toggle back to False when the `__iter__` method of the StreamingDataLoader completes.\n        self.restore = True\n\n        if isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset._set_use_streaming_dataloader(True)\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be a `StreamingDataset` or a `CombinedStreamingDataset`.\")"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if not state_dict:\n            return\n\n        if len(state_dict[\"dataset\"]) != len(self._datasets):\n            raise RuntimeError(f\"The provided state doesn't match the current number of datasets: {self._datasets}.\")\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            if str(dataset_idx) not in state_dict[\"dataset\"]:\n                raise RuntimeError(f\"The provided state doesn't contain the index {dataset_idx}.\")\n\n            dataset.load_state_dict(state_dict[\"dataset\"][str(dataset_idx)])\n\n        # Used to iterate over the sampler to avoid sampling the same samples\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[\"num_samples_yielded\"]"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return Dir(path=str(dir_path.path) if dir_path.path else None, url=str(dir_path.url) if dir_path.url else None)\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"`dir_path` must be a `Dir` or a string, got: {dir_path}\")\n\n    assert isinstance(dir_path, str)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=None, url=dir_path)\n\n    if dir_path.startswith(\"local:\"):\n        return Dir(path=None, url=dir_path)\n\n    dir_path = _resolve_time_template(dir_path)\n\n    dir_path_absolute = str(Path(dir_path).absolute().resolve())\n\n    if dir_path_absolute.startswith(\"/teamspace/studios/this_studio\"):\n        return Dir(path=dir_path_absolute, url=None)\n\n    if dir_path_absolute.startswith(\"/.project\"):\n        dir_path_absolute = dir_path\n\n    if dir_path_absolute.startswith(\"/teamspace/studios\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_studio(dir_path_absolute, dir_path_absolute.split(\"/\")[3], None)\n\n    if dir_path_absolute.startswith(\"/teamspace/s3_connections\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_s3_connections(dir_path_absolute)\n\n    if dir_path_absolute.startswith(\"/teamspace/datasets\") and len(dir_path_absolute.split(\"/\")) > 3:\n        return _resolve_datasets(dir_path_absolute)\n\n    return Dir(path=dir_path_absolute, url=None)"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        num_workers = num_workers or 1\n\n        # Only for non rank 0\n        if self.rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.01)\n            return\n\n        # Wait for all indexes to be available\n        is_done = False\n        while not is_done:\n            files = os.listdir(self._cache_dir)\n\n            # Return if the index already exists\n            if _INDEX_FILENAME in files:\n                return\n\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            # When using the Data Optimizer, we don't use multi processes.\n            is_done = len(index_files) == self._distributed_env.world_size * num_workers\n            sleep(0.01)\n\n        self._merge_no_wait(node_rank=node_rank)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The `lightning_sdk` is required.\")\n\n    lightning_skip_install = os.getenv(\"LIGHTNING_SKIP_INSTALL\", \"\")\n    if lightning_skip_install:\n        lightning_skip_install = f\" LIGHTNING_SKIP_INSTALL={lightning_skip_install} \"\n\n    lightning_branch = os.getenv(\"LIGHTNING_BRANCH\", \"\")\n    if lightning_branch:\n        lightning_branch = f\" LIGHTNING_BRANCH={lightning_branch} \"\n\n    studio = Studio()\n    job = studio._studio_api.create_data_prep_machine_job(\n        command or f\"cd {os.getcwd()} &&{lightning_skip_install}{lightning_branch} python {' '.join(sys.argv)}\",\n        name=name,\n        num_instances=num_nodes,\n        studio_id=studio._studio.id,\n        teamspace_id=studio._teamspace.id,\n        cluster_id=studio._studio.cluster_id,\n        machine=machine or studio._studio_api.get_machine(studio._studio.id, studio._teamspace.id),\n    )\n\n    has_printed = False\n\n    while True:\n        curr_job = studio._studio_api._client.lightningapp_instance_service_get_lightningapp_instance(\n            project_id=studio._teamspace.id, id=job.id\n        )\n        if not has_printed:\n            cloud_url = os.getenv(\"LIGHTNING_CLOUD_URL\", \"https://lightning.ai\").replace(\":443\", \"\")\n            job_url = f\"{cloud_url}/{studio.owner}/{studio._teamspace.name}\"\n            job_url += f\"/studios/{studio.name}/app?app_id=data-prep&job_name={curr_job.name}\"\n            print(f\"Find your job at {job_url}\")\n            has_printed = True\n\n        if curr_job.status.phase == \"LIGHTNINGAPP_INSTANCE_STATE_FAILED\":\n            raise RuntimeError(f\"job {curr_job.name} failed!\")\n\n        if curr_job.status.phase in [\"LIGHTNINGAPP_INSTANCE_STATE_STOPPED\", \"LIGHTNINGAPP_INSTANCE_STATE_COMPLETED\"]:\n            break\n\n        sleep(1)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        self._config = ChunksConfig.load(self._cache_dir, self._serializers, self._remote_input_dir, self._item_loader)\n        return self._config"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The Reader.read(...) method expects a chunked Index.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        if self._config and (self._config._remote_dir or self._config._compressor):\n            # Create and start the prepare chunks thread\n            if self._prepare_thread is None and self._config:\n                self._prepare_thread = PrepareChunksThread(\n                    self._config, self._item_loader, self._distributed_env, self._max_cache_size\n                )\n                self._prepare_thread.start()\n                if index.chunk_indexes:\n                    self._prepare_thread.download(index.chunk_indexes)\n\n            # If the chunk_index is new, request for it to be downloaded.\n            if index.chunk_index != self._last_chunk_index:\n                assert self._prepare_thread\n                self._prepare_thread.download([index.chunk_index])\n\n            if self._last_chunk_index is None:\n                self._last_chunk_index = index.chunk_index\n\n        # Fetch the element\n        chunk_filepath, begin, _ = self.config[index]\n        item = self._item_loader.load_item_from_chunk(index.index, index.chunk_index, chunk_filepath, begin)\n\n        # We need to request deletion after the latest element has been loaded.\n        # Otherwise, this could trigger segmentation fault error depending on the item loader used.\n        if self._config and self._config._remote_dir and index.chunk_index != self._last_chunk_index:\n            assert self._prepare_thread\n            assert self._last_chunk_index is not None\n\n            # inform the chunk has been completely consumed\n            self._prepare_thread.delete([self._last_chunk_index])\n\n            # track the new chunk index as the latest one\n            self._last_chunk_index = index.chunk_index\n\n        if index.is_last_index and self._prepare_thread:\n            # inform the thread it is time to stop\n            self._prepare_thread.stop()\n            self._prepare_thread = None\n\n        return item"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    chunk_indexes_per_nodes: Any = [[] for _ in range(distributed_env.num_nodes)]\n    process_per_node = distributed_env.world_size // distributed_env.num_nodes\n    for rank, chunks_per_rank in enumerate(chunks_per_ranks):\n        chunk_indexes_per_nodes[0 if distributed_env.num_nodes == 1 else rank // process_per_node].extend(\n            chunks_per_rank\n        )\n\n    # shuffle the chunks associated to the node\n    for i in range(len(chunk_indexes_per_nodes)):\n        # permute the indexes within the node\n        chunk_indexes_per_nodes[i] = np.random.RandomState(seed=seed + current_epoch).permutation(\n            chunk_indexes_per_nodes[i]\n        )\n\n    return [index for chunks in chunk_indexes_per_nodes for index in chunks]"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) == 0:\n        # Check whether the second element has any input_path\n        indexed_paths = _get_indexed_paths(inputs[1])\n        if len(indexed_paths) == 0:\n            return None\n\n        # Every element should have filepaths if any contains one.\n        raise ValueError(f\"The provided item {inputs[0]} didn't contain any filepaths.\")\n\n    absolute_path = str(Path(list(indexed_paths.values())[0]).resolve())\n\n    if \"/.project\" in absolute_path:\n        return \"/\" + os.path.join(*str(list(indexed_paths.values())[0]).split(\"/\")[:4])\n\n    return \"/\" + os.path.join(*str(absolute_path).split(\"/\")[:4])"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    optimize_dns(enable)\n    try:\n        yield\n        optimize_dns(False)  # always disable the optimize DNS\n    except Exception as e:\n        optimize_dns(False)  # always disable the optimize DNS\n        raise e"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = sum([(interval[-1] - interval[0]) for interval in chunk_intervals])\n    num_items_per_ranks: List[int] = [\n        num_items // distributed_env.world_size + num_items % distributed_env.world_size\n        if rank == distributed_env.world_size - 1 and not drop_last\n        else num_items // distributed_env.world_size\n        for rank in range(distributed_env.world_size)\n    ]\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[List[int]]] = [[] for _ in range(distributed_env.world_size)]\n\n    # 4. Assign the chunk & intervals to each rank\n    for chunk_index, chunk_interval in zip(indexes, chunk_intervals):\n        rank = 0\n\n        while True:\n            if rank == len(num_items_per_ranks):\n                break\n\n            items_left_to_assign = num_items_per_ranks[rank]\n\n            if items_left_to_assign == 0:\n                rank += 1\n                continue\n\n            items_in_chunk = chunk_interval[-1] - chunk_interval[0]\n\n            if items_in_chunk == 0:\n                break\n\n            if items_in_chunk > items_left_to_assign:\n                chunks_per_ranks[rank].append(chunk_index)\n                begin, end = chunk_interval\n                intervals_per_ranks[rank].append([begin, begin + items_left_to_assign])\n                chunk_interval = (begin + items_left_to_assign, end)\n                num_items_per_ranks[rank] = 0\n                rank += 1\n            else:\n                chunks_per_ranks[rank].append(chunk_index)\n                intervals_per_ranks[rank].append(chunk_interval)\n                num_items_per_ranks[rank] -= items_in_chunk\n                break\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device and self._device is None:\n            self._find_device()\n\n        kwargs: Dict[str, Any] = {}\n\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        if isinstance(self._fn, (FunctionType, partial)):\n            self._fn(item_metadata, output_dir, **kwargs)\n\n        elif callable(self._fn):\n            self._fn.__call__(item_metadata, output_dir, **kwargs)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if \"the HeadObject operation: Not Found\" in str(e):\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if chunk_size is None and chunk_bytes is None:\n        raise ValueError(\"Either `chunk_size` or `chunk_bytes` needs to be defined.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            reader=reader,\n        )\n\n        with optimize_dns_context(True):\n            data_processor.run(\n                LambdaDataChunkRecipe(\n                    fn,\n                    inputs,\n                    chunk_size=chunk_size,\n                    chunk_bytes=chunk_bytes,\n                    compression=compression,\n                )\n            )\n        return None\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaDataTransformRecipe(fn, inputs))\n    return _execute(\n        f\"data-prep-map-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            queue_out.put(None)\n            return\n\n        # 4. Unpack\n        index, paths = r\n\n        # 5. Check whether all the files are already downloaded\n        if input_dir.path and all(\n            os.path.exists(p.replace(input_dir.path, cache_dir) if input_dir else p) for p in paths\n        ):\n            queue_out.put(index)\n            continue\n\n        if input_dir.url is not None or input_dir.path is not None:\n            if input_dir.url:\n                # 6. Wait for the removers to catch up when we are downloading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Download all the required paths to unblock the current index\n            for path in paths:\n                if input_dir.path:\n                    local_path = path.replace(input_dir.path, cache_dir)\n\n                if input_dir.url and input_dir.path:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    dirpath = os.path.dirname(local_path)\n\n                    os.makedirs(dirpath, exist_ok=True)\n\n                    with open(local_path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n                elif os.path.isfile(path):\n                    if not path.startswith(\"/teamspace/studios/this_studio\"):\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copyfile(path, local_path)\n                else:\n                    raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        # 7. Inform the worker the current files are available\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        tmpdir = None\n\n        if isinstance(data, str) or data is None:\n            local_filepath = data\n        else:\n            tmpdir, local_filepath = data\n\n        # Terminate the process if we received a termination signal\n        if local_filepath is None:\n            return\n\n        # Upload the file to the target cloud storage\n        if not local_filepath.startswith(cache_dir):\n            local_filepath = os.path.join(cache_dir, local_filepath)\n\n        if obj.scheme == \"s3\":\n            try:\n                if tmpdir is None:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n                else:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), local_filepath.replace(tmpdir, \"\")[1:])\n\n                s3.client.upload_file(\n                    local_filepath,\n                    obj.netloc,\n                    output_filepath,\n                )\n            except Exception as e:\n                print(e)\n\n        elif output_dir.path:\n            if tmpdir is None:\n                output_filepath = os.path.join(output_dir.path, os.path.basename(local_filepath))\n            else:\n                output_filepath = os.path.join(output_dir.path, local_filepath.replace(tmpdir, \"\")[1:])\n\n            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n            shutil.move(local_filepath, output_filepath)\n        else:\n            raise ValueError(f\"The provided {output_dir.path} isn't supported.\")\n\n        # Inform the remover to delete the file\n        if remove_queue and os.path.exists(local_filepath):\n            remove_queue.put([local_filepath])"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    for worker_id, size in worker_weights.items():\n        if worker_id not in worker_ids_this_node:\n            continue\n\n        if file_size:\n            print(f\"Worker {worker_id} gets {size / 1e6:.1f} MB ({len(worker_items[worker_id])} files)\")\n        else:\n            print(f\"Worker {worker_id} gets ({len(worker_items[worker_id])}) items for a total weight of {size}.\")\n\n    return [np.random.permutation(worker_items[worker_id]).tolist() for worker_id in worker_ids_this_node]\n\n\ndef _get_num_bytes(item: Any, base_path: str) -> int:\n    flattened_item, _ = tree_flatten(item)\n\n    num_bytes = 0\n    for element in flattened_item:\n        if isinstance(element, str):"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "\n    from typing import List, Any\n    import os\n    total_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = total_nodes * num_workers\n\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return result\n\n\ndef _map_items_to_workers_weighted(\n    num_workers: int,\n    user_items: List[Any],\n    weights: Optional[List[int]] = None,\n    file_size: bool = True,\n) -> List[List[Any]]:"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "            item_sizes.append(future.result())\n    return item_sizes\n\n\ndef _to_path(element: str) -> str:\n    return element if _IS_IN_STUDIO and element.startswith(\"/teamspace\") else str(Path(element).resolve())\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if not isinstance(element, str):"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "        if element.startswith(input_dir):\n            return True\n\n    return os.path.exists(element)\n\n\nclass BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0 and n_neurons > 0\n\n        if self.tcnn is True:\n            import tinycudann as tcnn\n            otype = \"FullyFusedMLP\"\n            if n_neurons > 128:\n                otype = \"CutlassMLP\"\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                network_config={\n                    \"otype\": otype,\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n                seed=self._get_seed(),\n            )\n\n        # PyTorch\n        model_list = []\n        # hidden layers\n        in_features = n_input_dims\n        for i in range(n_layers - 1):\n            model_list += self._get_torch_layer(in_features, n_neurons, activation)\n            in_features = n_neurons  # next layer's in_features\n        # output layer\n        model_list += self._get_torch_layer(in_features, n_output_dims, output_activation)\n\n        return nn.Sequential(*model_list)"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        if signal.ndim != 1:\n            raise GeometryRefinementError(\"Smoothing._rolling_median only works for 1d arrays.\")\n\n        stacked_signals: List[np.ndarray] = []\n        for i in range(-kernel_offset, kernel_offset + 1):\n            stacked_signals.append(np.roll(signal, i))\n        stacked_signals = np.stack(stacked_signals)\n\n        rolling_median = np.median(stacked_signals, axis=0)\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    half_codewidth = []\n\n    for probe_code, gallery_code in zip(template_probe.iris_codes, template_gallery.iris_codes):\n        if probe_code.shape != gallery_code.shape:\n            raise MatcherError(\"probe and gallery iris codes are of different sizes\")\n        if (probe_code.shape[1] % 2) != 0:\n            raise MatcherError(\"number of columns of iris codes need to be even\")\n        half_codewidth.append(int(probe_code.shape[1] / 2))\n\n    if weights:\n        for probe_code, w in zip(template_probe.iris_codes, weights):\n            if probe_code.shape != w.shape:\n                raise MatcherError(\"weights table and iris codes are of different sizes\")\n\n    if nm_dist:\n        if weights:\n            sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n                np.sum([np.size(a) for a in template_probe.iris_codes]), half_codewidth, weights\n            )\n        else:\n            sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n                np.sum([np.size(a) for a in template_probe.iris_codes]), half_codewidth\n            )\n\n    # Calculate the Hamming distance between probe and gallery template.\n    match_dist = 1\n    match_rot = 0\n    for shiftby in range(-rotation_shift, rotation_shift + 1):\n        irisbits = [\n            np.roll(probe_code, shiftby, axis=1) != gallery_code\n            for probe_code, gallery_code in zip(template_probe.iris_codes, template_gallery.iris_codes)\n        ]\n        maskbits = [\n            np.roll(probe_code, shiftby, axis=1) & gallery_code\n            for probe_code, gallery_code in zip(template_probe.mask_codes, template_gallery.mask_codes)\n        ]\n\n        if weights:\n            irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n                irisbits, maskbits, half_codewidth, weights\n            )\n        else:\n            irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n                irisbits, maskbits, half_codewidth\n            )\n        maskbitcount = maskbitcount_top + maskbitcount_bot\n\n        if maskbitcount == 0:\n            continue\n\n        if nm_dist:\n            normdist_top = (\n                normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n                if maskbitcount_top > 0\n                else 1\n            )\n            normdist_bot = (\n                normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n                if maskbitcount_bot > 0\n                else 1\n            )\n            w_top = np.sqrt(maskbitcount_top)\n            w_bot = np.sqrt(maskbitcount_bot)\n            Hdist = (\n                normalized_HD((irisbitcount_top + irisbitcount_bot), maskbitcount, sqrt_totalbitcount, nm_dist) / 2\n                + (normdist_top * w_top + normdist_bot * w_bot) / (w_top + w_bot) / 2\n            )\n        else:\n            Hdist = (irisbitcount_top + irisbitcount_bot) / maskbitcount\n\n        if (Hdist < match_dist) or (Hdist == match_dist and shiftby == 0):\n            match_dist = Hdist\n            match_rot = shiftby\n\n    return match_dist, match_rot"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        np.random.seed(142857)\n\n        bisectors_first_points = np.empty([0, 2])\n        bisectors_second_points = np.empty([0, 2])\n        for _ in range(self.params.max_iterations):\n            random_indices = np.random.choice(len(polygon), size=(self.params.num_bisectors, 2))\n\n            first_drawn_points = polygon[random_indices[:, 0]]\n            second_drawn_points = polygon[random_indices[:, 1]]\n\n            norms = np.linalg.norm(first_drawn_points - second_drawn_points, axis=1)\n            mask = norms > min_distance_between_sector_points_in_px\n\n            bisectors_first_points = np.vstack([bisectors_first_points, first_drawn_points[mask]])\n            bisectors_second_points = np.vstack([bisectors_second_points, second_drawn_points[mask]])\n\n            if len(bisectors_first_points) >= self.params.num_bisectors:\n                break\n        else:\n            raise EyeCentersEstimationError(\n                \"Not able to find enough random pairs of points on the arc with a large enough distance!\"\n            )\n\n        bisectors_first_points = bisectors_first_points[: self.params.num_bisectors]\n        bisectors_second_points = bisectors_second_points[: self.params.num_bisectors]\n\n        bisectors_center = (bisectors_first_points + bisectors_second_points) / 2\n\n        # Flip xs with ys and flip sign of on of them to create a 90deg rotation\n        inv_bisectors_center_slope = np.fliplr(bisectors_second_points - bisectors_first_points)\n        inv_bisectors_center_slope[:, 1] = -inv_bisectors_center_slope[:, 1]\n\n        # Add perpendicular vector to center and normalize\n        norm = np.linalg.norm(inv_bisectors_center_slope, axis=1)\n        inv_bisectors_center_slope[:, 0] /= norm\n        inv_bisectors_center_slope[:, 1] /= norm\n\n        first_bisectors_point = bisectors_center - inv_bisectors_center_slope\n        second_bisectors_point = bisectors_center + inv_bisectors_center_slope\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback_func in self._callbacks:\n            callback_func.on_execute_start(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback_func in self._callbacks:\n            callback_func.on_execute_end(result)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract input type hints and output type hint\n        input_type_hints = {k: v for k, v in type_hints.items() if k in sig.parameters}\n        output_type_hint = type_hints.get('return')\n\n        # Fetch the docstring\n        docstring = func_object.__doc__.strip() if func_object.__doc__ else \"\"\n\n        def get_class_definition(class_type):\n            \"\"\"Helper function to get class definition source if not a built-in type\"\"\"\n            if hasattr(class_type, \"__origin__\"):  # Check if it's a generic type\n                origin_type = class_type.__origin__\n                if origin_type is Literal:  # Handle Literal case\n                    return [literal for literal in class_type.__args__]\n                elif hasattr(class_type, \"__args__\"):  # Access inner types\n                    return [get_class_definition(arg) for arg in class_type.__args__ if arg is not None]\n            elif inspect.isclass(class_type) and class_type.__module__ != \"builtins\":\n                return get_source(class_type)\n            return class_type.__name__\n\n        # Extract class definitions for input and output types\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n        # if inspect.isclass(output_type_hint) and issubclass(output_type_hint, Embedding):\n        #     output_class_definition = None\n        # else:\n        #     output_class_definition = get_class_definition(output_type_hint)\n        output_class_definition = None\n        function_type = FunctionType.SYMBOLIC\n        # check if the output type hint is a class or a subclass of a Union\n        if inspect.isclass(output_type_hint) or (hasattr(output_type_hint, \"__origin__\") and\n                                                 output_type_hint.__origin__ == Union):\n            if (hasattr(output_type_hint, \"__origin__\") and output_type_hint.__origin__ == Union): # it's a union\n                # get all the types in the union\n                union_types = output_type_hint.__args__\n                output_type_descriptions = {}\n                for output_type in union_types:\n                    # check if it is a class Nonetype\n                    if output_type is type(None):\n                        output_type_descriptions[\"NoneType\"] = \"None\"\n                    elif inspect.isclass(output_type):\n                        # Check if the base class of the output type hint is Embedding\n                        base_class = get_origin(output_type) or output_type\n                        if issubclass(base_class, Embedding):\n                            output_class_definition = None\n                            function_type = FunctionType.EMBEDDABLE\n                            break\n                        else:\n                            class_type_description = get_class_definition(output_type)\n                            if isinstance(class_type_description,str):\n                                class_type_description = class_type_description.replace('\"', \"'\") # less horrible prompt formatting when dump to json\n                            output_type_descriptions[output_type.__name__] = class_type_description\n                output_class_definition = f\"Union of following classes {json.dumps(output_type_descriptions)}\"\n\n            else: # it's a class\n                # Check if the base class of the output type hint is Embedding\n                base_class = get_origin(output_type_hint) or output_type_hint\n                if issubclass(base_class, Embedding):\n                    output_class_definition = None\n                    function_type = FunctionType.EMBEDDABLE\n                else:\n                    output_class_definition = get_class_definition(output_type_hint)\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            type=function_type\n        )"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        self.bit_array = self.persistence.load()\n\n        length_in_bytes = int(len(self.bit_array)/8)\n        expected_length = math.ceil(self.size / 8)\n        if length_in_bytes != expected_length:\n            logging.warning(\"Bit array length does not match expected size, and so might be corrupted. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n\n            #print(f\"Lookup: Seed={seed}, Digest={index}, BitValue={self.bit_array[index]}\")\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_config(json_dict[\"distilled_model\"], DISTILLED_MODEL)\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict and len(json_dict[\"teacher_models\"]) > 0:\n            self.teacher_models = [config_factory.create_config(teacher_model, TEACHER_MODEL) for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        temperature = kwargs.get(\"temperature\", 0.1)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\")\n        # check if there are any generation parameters that are not supported\n        unsupported_params = [param for param in kwargs.keys() if param not in LLM_GENERATION_PARAMETERS]\n        if len(unsupported_params) > 0:\n            # log warning\n            logging.warning(f\"Unused generation parameters sent as input: {unsupported_params}.\"\\\n                             f\"For OpenAI, only the following parameters are supported: {LLM_GENERATION_PARAMETERS}\")\n        params = {\n            \"model\": model.model_name,\n            \"temperature\": temperature,\n            \"max_tokens\": max_new_tokens,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n        }\n        if model.parsing_helper_tokens[\"start_token\"]:\n            prompt += model.parsing_helper_tokens[\"start_token\"]\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": system_message\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n        params[\"messages\"] = messages\n\n        counter = 0\n        choice = None\n        # initiate response so exception logic doesnt error out when checking for error in response\n        response = {}\n        while counter <= 5:\n            try:\n                openai_headers = {\n                    \"Authorization\": f\"Bearer {self.api_key}\",\n                    \"Content-Type\": \"application/json\",\n                }\n                response = requests.post(\n                    OPENAI_URL, headers=openai_headers, json=params, timeout=50\n                )\n                response = response.json()\n                choice = response[\"choices\"][0][\"message\"][\"content\"].strip(\"'\")\n                break\n            except Exception as e:\n                if (\"error\" in response and\n                        \"code\" in response[\"error\"] and\n                        response[\"error\"][\"code\"] == 'invalid_api_key'):\n                    raise Exception(f\"The supplied OpenAI API key {self.api_key} is invalid\")\n                if counter == 5:\n                    raise Exception(f\"OpenAI API failed to generate a response: {e}\")\n                counter += 1\n                time.sleep(2 ** counter)\n                continue\n\n        if not choice:\n            raise Exception(\"OpenAI API failed to generate a response\")\n        \n        if model.parsing_helper_tokens[\"end_token\"]:\n            # remove the end token from the choice\n            choice = choice.split(model.parsing_helper_tokens[\"end_token\"])[0]\n            # check if starting token is in choice\n            if model.parsing_helper_tokens[\"start_token\"] in choice:\n                # remove the starting token from the choice\n                choice = choice.split(model.parsing_helper_tokens[\"start_token\"])[-1]\n        return choice"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0]), atol=1e-5):\n        raise ValueError(\n            \"The distance matrix must have diagonal elements close to zeros\"\n        )"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        f = str(function_description.__dict__.__repr__())\n\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        is_distilled_model = distilled_model.model_name != \"\"\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f,\n                                                                                                       distilled_model)\n        if func_hash not in self.initialized_functions:\n            # initialise the initialized_functions dict\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # no examples needed, using a finetuned model. Dont save to finetune dataset\n        if is_distilled_model and suitable_for_distillation:\n            prompt = self.construct_prompt(f, args, kwargs, [], distilled_model)\n            return prompt, distilled_model, suitable_for_distillation, True\n\n        else:\n            aligns = self.function_modeler.get_symbolic_alignments(function_description.__hash__(), max=16)\n            examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                 aligns]\n            \n            # update the examples in the initialized_functions dict\n            self.initialized_functions[func_hash][\"examples\"] = examples\n\n            examples_token_count = sum([approximate_token_count(example) for example in examples])\n            generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n            model = self.choose_model_from_tokens(teacher_models,\n                                                  examples_token_count + input_prompt_token_count + generation_tokens,\n                                                  len(examples))\n            if model:\n                examples_with_parsing_tokens = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput:{model.parsing_helper_tokens['start_token']}{align['output']}{model.parsing_helper_tokens['end_token']}\" for align in\n                 aligns]\n                prompt = self.construct_prompt(f, args, kwargs, examples_with_parsing_tokens, model)\n                return prompt, model, suitable_for_distillation, False\n            else:\n                raise ValueError(\n                    \"The input content and align statements combined are too long, please shorten it. The maximum currently allowed token limit is 32000\")"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    # Around 100 times faster than checking eigenvalues with np.linalg.eigh\n    if is_cholesky_dec(cov) and is_positive_definite(cov):\n        return cov\n\n    corr, std = cov_to_corr(cov)\n\n    if higham:\n        eps = np.finfo(np.float64).eps * 5\n        diff = np.zeros(corr.shape)\n        x = corr.copy()\n        for _ in range(higham_max_iteration):\n            x_adj = x - diff\n            eig_vals, eig_vecs = np.linalg.eigh(x_adj)\n            x = eig_vecs * np.maximum(eig_vals, eps) @ eig_vecs.T\n            diff = x - x_adj\n            np.fill_diagonal(x, 1)\n            cov = corr_to_cov(x, std)\n            if is_cholesky_dec(cov) and is_positive_definite(cov):\n                break\n        else:\n            raise ValueError(\"Unable to find the nearest positive definite matrix\")\n    else:\n        eig_vals, eig_vecs = np.linalg.eigh(corr)\n        # Clipping the eigenvalues with a value smaller than 1e-13 can cause scipy to\n        # consider the matrix non-psd is some corner cases (see test/test_stats.py)\n        x = eig_vecs * np.maximum(eig_vals, _CLIPPING_VALUE) @ eig_vecs.T\n        x, _ = cov_to_corr(x)\n        cov = corr_to_cov(x, std)\n\n    return cov"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    schemas = [\n        ((str, bytes), IdentitySchema),\n        (list, ListSchema),\n        (tuple, TupleSchema),\n        (collections.abc.Mapping, DictSchema),\n        (Instances, InstancesSchema),\n        ((Boxes, ROIMasks), TensorWrapSchema),\n    ]\n    for klass, schema in schemas:\n        if isinstance(obj, klass):\n            F = schema\n            break\n    else:\n        F = IdentitySchema\n\n    return F.flatten(obj)"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"`{names[0]}` must be a 2D array, got {groups.ndim}D array instead.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"`{names[1]}` must be a 1D array, got {equations.ndim}D array instead.\"\n        )\n\n    n_equations = len(equations)\n    n_assets = groups.shape[1]\n    a = np.zeros((n_equations, n_assets))\n    b = np.zeros(n_equations)\n    for i, string in enumerate(equations):\n        try:\n            left, right = _string_to_equation(\n                groups=groups,\n                string=string,\n                sum_to_one=sum_to_one,\n            )\n            a[i] = left\n            b[i] = right\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise\n            warnings.warn(str(e), stacklevel=2)\n    return a, b"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with tempfile.TemporaryDirectory(prefix=\"detectron2\") as dir, tempfile.NamedTemporaryFile(\n        mode=\"w\", encoding=\"utf-8\", suffix=\".py\", dir=dir, delete=False\n    ) as f:\n        try:\n            # Objects that use Instances should not reuse previously-compiled\n            # results in cache, because `Instances` could be a new class each time.\n            _clear_jit_cache()\n\n            cls_name, s = _gen_instance_module(fields)\n            f.write(s)\n            f.flush()\n            f.close()\n\n            module = _import(f.name)\n            new_instances = getattr(module, cls_name)\n            _ = torch.jit.script(new_instances)\n            # let torchscript think Instances was scripted already\n            Instances.__torch_script_class__ = True\n            # let torchscript find new_instances when looking for the jit type of Instances\n            Instances._jit_override_qualname = torch._jit_internal._qualified_name(new_instances)\n\n            _add_instances_conversion_methods(new_instances)\n            yield new_instances\n        finally:\n            try:\n                del Instances.__torch_script_class__\n                del Instances._jit_override_qualname\n            except AttributeError:\n                pass\n            sys.modules.pop(module.__name__)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n\n        # work around this bug: https://github.com/python-pillow/Pillow/issues/3973\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box(np.array([bbox]))[0].clip(min=0)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # each instance contains 1 or more polygons\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        elif isinstance(segm, dict):\n            # RLE\n            mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            assert tuple(mask.shape[:2]) == image_size\n            annotation[\"segmentation\"] = mask\n        else:\n            raise ValueError(\n                \"Cannot transform segmentation of type '{}'!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict.\".format(type(segm))\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        coords = np.asarray(coords, dtype=float)\n        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    old_train = model.training\n    model.eval()\n    ret = FlopCountAnalysis(model, inputs).by_operator()\n    model.train(old_train)\n    return {k: v / 1e9 for k, v in ret.items()}"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img) == 0 or self.angle % 360 == 0:\n            return img\n        assert img.shape[:2] == (self.h, self.w)\n        interp = interp if interp is not None else self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(\n                self._create_grayscale_image(\n                    (predictions.pred_masks.any(dim=0) > 0).numpy()\n                    if predictions.has(\"pred_masks\")\n                    else None\n                )\n            )\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha,\n        )\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        # buf = io.BytesIO()  # works for cairo backend\n        # canvas.print_rgba(buf)\n        # width, height = self.width, self.height\n        # s = buf.getvalue()\n\n        buffer = np.frombuffer(s, dtype=\"uint8\")\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [\n                BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\n                if len(x[\"bbox\"]) == 4\n                else x[\"bbox\"]\n                for x in annos\n            ]\n\n            colors = None\n            category_ids = [x[\"category_id\"] for x in annos]\n            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                    for c in category_ids\n                ]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = _create_text_labels(\n                category_ids,\n                scores=None,\n                class_names=names,\n                is_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n            )\n            self.overlay_instances(\n                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        if pan_seg is None and \"pan_seg_file_name\" in dic:\n            with PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\n                pan_seg = Image.open(f)\n                pan_seg = np.asarray(pan_seg)\n                from panopticapi.utils import rgb2id\n\n                pan_seg = rgb2id(pan_seg)\n        if pan_seg is not None:\n            segments_info = dic[\"segments_info\"]\n            pan_seg = torch.tensor(pan_seg)\n            self.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        has_valid_segment = False\n        binary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n\n        if not mask.has_holes:\n            # draw polygons for regular masks\n            for segment in mask.polygons:\n                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\n                if area < (area_threshold or 0):\n                    continue\n                has_valid_segment = True\n                segment = segment.reshape(-1, 2)\n                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\n        else:\n            # TODO: Use Path/PathPatch to draw vector graphics:\n            # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\n            rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n            rgba[:, :, :3] = color\n            rgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\n            has_valid_segment = True\n            self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None and has_valid_segment:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        input = convert_scripted_instances(input)\n    if not isinstance(other, Instances):\n        other = convert_scripted_instances(other)\n\n    if not msg:\n        msg = \"Two Instances are different! \"\n    else:\n        msg = msg.rstrip() + \" \"\n\n    size_error_msg = msg + f\"image_size is {input.image_size} vs. {other.image_size}!\"\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), size_error_msg\n    else:\n        assert input.image_size == other.image_size, size_error_msg\n    fields = sorted(input.get_fields().keys())\n    fields_other = sorted(other.get_fields().keys())\n    assert fields == fields_other, msg + f\"Fields are {fields} vs {fields_other}!\"\n\n    for f in fields:\n        val1, val2 = input.get(f), other.get(f)\n        if isinstance(val1, (Boxes, ROIMasks)):\n            # boxes in the range of O(100) and can have a larger tolerance\n            assert torch.allclose(val1.tensor, val2.tensor, atol=100 * rtol), (\n                msg + f\"Field {f} differs too much!\"\n            )\n        elif isinstance(val1, torch.Tensor):\n            if val1.dtype.is_floating_point:\n                mag = torch.abs(val1).max().cpu().item()\n                assert torch.allclose(val1, val2, atol=mag * rtol), (\n                    msg + f\"Field {f} differs too much!\"\n                )\n            else:\n                assert torch.equal(val1, val2), msg + f\"Field {f} is different!\"\n        else:\n            raise ValueError(f\"Don't know how to compare type {type(val1)}\")"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = (\n            cat([p.gt_classes for p in proposals], dim=0) if len(proposals) else torch.empty(0)\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposals):\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals should not require gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation.\n            # Here we just use proposal_boxes as an arbitrary placeholder because its\n            # value won't be used in self.box_reg_loss().\n            gt_boxes = cat(\n                [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n                dim=0,\n            )\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": self.box_reg_loss(\n                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER_HEADS.TRACKER_NAME\n    tracker_class = TRACKER_HEADS_REGISTRY.get(name)\n    return tracker_class(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        x1 = pred_ctr_x - 0.5 * pred_w\n        y1 = pred_ctr_y - 0.5 * pred_h\n        x2 = pred_ctr_x + 0.5 * pred_w\n        y2 = pred_ctr_y + 0.5 * pred_h\n        pred_boxes = torch.stack((x1, y1, x2, y2), dim=-1)\n        return pred_boxes.reshape(deltas.shape)"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output_image = self.general_ins({'img': image})\n        if anno_type is not None:\n            if isinstance(anno_type, str) and anno_type in output_image:\n                return output_image[anno_type]\n            else:\n                return {\n                    tp: output_image[tp]\n                    for tp in anno_type if tp in output_image\n                }\n        else:\n            return output_image"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        url_scores: dict[str, float] = {}\n        for kw in keywords:\n            kw_urls_score = self.bm25(kw)\n            url_scores = update_url_scores(url_scores, kw_urls_score)\n        return url_scores"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        h, w = box_size\n\n        # normalize angles to be within (-180, 180] degrees\n        self.normalize_angles()\n\n        idx = torch.where(torch.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]\n\n        # convert to (x1, y1, x2, y2)\n        x1 = self.tensor[idx, 0] - self.tensor[idx, 2] / 2.0\n        y1 = self.tensor[idx, 1] - self.tensor[idx, 3] / 2.0\n        x2 = self.tensor[idx, 0] + self.tensor[idx, 2] / 2.0\n        y2 = self.tensor[idx, 1] + self.tensor[idx, 3] / 2.0\n\n        # clip\n        x1.clamp_(min=0, max=w)\n        y1.clamp_(min=0, max=h)\n        x2.clamp_(min=0, max=w)\n        y2.clamp_(min=0, max=h)\n\n        # convert back to (xc, yc, w, h)\n        self.tensor[idx, 0] = (x1 + x2) / 2.0\n        self.tensor[idx, 1] = (y1 + y2) / 2.0\n        # make sure widths and heights do not increase due to numerical errors\n        self.tensor[idx, 2] = torch.min(self.tensor[idx, 2], x2 - x1)\n        self.tensor[idx, 3] = torch.min(self.tensor[idx, 3], y2 - y1)"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stat = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for type_ in stat.keys():\n            stat[type_] = sum([obj['type']==type_ for obj in self.data])\n        return stat"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for img_id in range(len(dt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n\n        # parse gt annotations\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(\n                gt_anno['gt_boxes_upright_depth'],\n                box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1],\n                origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n\n    rec, prec, ap = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label]\n                      for label in ap[0].keys()] + ['Overall']]\n\n    for i, iou_thresh in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(\n                ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(\n            np.mean(list(ap[i].values())))\n\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(\n                rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n\n    return ret_dict"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes,\n                              DepthInstance3DBoxes, LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\"'\n                         f' are supported, got {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      await self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        code = f\"{user_code}{test_code}\"\n        buffer = io.StringIO(code)\n\n        # This produces a stream of TokenInfos, example:\n        # TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 3), end=(4, 4), line='\"\"\"\\n'),\n        # TokenInfo(type=62 (NL), string='\\n', start=(5, 0), end=(5, 1), line='\\n')\n        # See https://docs.python.org/3/library/tokenize.html#tokenize.tokenize for more details\n        tokens = list(tokenize.generate_tokens(buffer.readline))\n\n        # Find all lines that are followed by a comment # expect-type-error\n        expect_error_line_numbers = [\n            token.start[0]\n            for token in tokens\n            if token.type == tokenize.COMMENT\n            and token.string[1:].strip() == cls.EXPECT_ERROR_COMMENT\n        ]\n        # Tracks whether an expected error has been reported by type checker.\n        error_line_seen_in_err_msg: dict[int, bool] = {\n            lineno: False for lineno in expect_error_line_numbers\n        }\n\n        with tempfile.NamedTemporaryFile(suffix=\".py\") as temp:\n            temp.write(code.encode())\n            temp.flush()\n            # TODO: switch to json output to simplify output parsing.\n            # https://microsoft.github.io/pyright/#/command-line?id=json-output\n            raw_result = subprocess.run(\n                [\"pyright\", \"--pythonversion\", \"3.12\", temp.name],\n                capture_output=True,\n                text=True,\n            )\n            stdout, stderr = raw_result.stdout, raw_result.stderr\n            if stderr:\n                return TypeCheckResult(message=stderr, passed=False)\n        error_lines: list[str] = []\n\n        # Substract lineno in merged code by lineno_delta, so that the lineno in\n        # error message matches those in the test code editor. Fixed #20.\n        lineno_delta = len(user_code.splitlines())\n        for line in stdout.splitlines():\n            m = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if m is None:\n                continue\n            line_number, message = int(m.group(1)), m.group(2)\n            if line_number in error_line_seen_in_err_msg:\n                # Each reported error should be attached to a specific line,\n                # If it is commented with # expect-type-error, let it pass.\n                error_line_seen_in_err_msg[line_number] = True\n                continue\n            # Error could be thrown from user code too, in which case delta shouldn't be applied.\n            error_lines.append(\n                f\"{line_number if line_number <= lineno_delta else line_number - lineno_delta}:{message}\"\n            )\n\n        # If there are any lines that are expected to fail but not reported by pyright,\n        # they should be considered as errors.\n        for line_number, seen in error_line_seen_in_err_msg.items():\n            if not seen:\n                error_lines.append(\n                    f\"{line_number - lineno_delta}: error: Expected type error but instead passed\"\n                )\n\n        passed = len(error_lines) == 0\n        if passed:\n            error_lines.append(\"\\nAll tests passed\")\n        else:\n            error_lines.append(f\"\\nFound {len(error_lines)} errors\")\n\n        return TypeCheckResult(message=\"\\n\".join(error_lines), passed=passed)"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn,\n                          fw_compiler=get_compiler_fn(\"Forward Code:\"),\n                          bw_compiler=get_compiler_fn(\"Backward Code:\"))\n    else:\n        return aot_function(fn,\n                            fw_compiler=get_compiler_fn(\"Forward Code:\"),\n                            bw_compiler=get_compiler_fn(\"Backward Code:\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_path = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_path):\n        raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n    trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n    config_yaml_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_yaml_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(yaml_dict, f)\n    return yaml_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    lock = threading.Lock()\n    traced_modules = {}\n\n    name = getattr(func, '__name__', func.__class__.__name__)\n    wrapped = func.forward if isinstance(func, torch.nn.Module) else func\n    module_to_be_traced = to_module(wrapped)\n\n    @functools.wraps(wrapped)\n    def wrapper(*args, **kwargs):\n        nonlocal lock, traced_modules\n        key = (module_to_be_traced.training, hash_arg(args), hash_arg(kwargs))\n        traced_module = traced_modules.get(key)\n        if traced_module is None:\n            with lock:\n                traced_module = traced_modules.get(key)\n                if traced_module is None:\n                    logger.info(f'Tracing {name}')\n                    traced_m, call_helper = trace_with_kwargs(\n                        module_to_be_traced, args, kwargs, **kwargs_)\n                    if ts_compiler is not None:\n                        if 'call_helper' in inspect.signature(\n                                ts_compiler).parameters:\n                            traced_m = ts_compiler(traced_m, call_helper, args,\n                                                   kwargs)\n                        else:\n                            converted_args = call_helper(\n                                traced_m).convert_inputs(args, kwargs)\n                            traced_m = ts_compiler(traced_m, converted_args)\n                    traced_module = call_helper(traced_m)\n                    traced_modules[key] = traced_module\n        return traced_module(*args, **kwargs)\n\n    if isinstance(func, torch.nn.Module):\n        wrapper.__self__ = func\n    elif hasattr(func, '__self__'):\n        wrapper.__self__ = func.__self__\n    wrapper._cached = traced_modules\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n    retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n    save_dir = os.path.join(node_line_dir, \"retrieval\")  # node name\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    def run_and_save(input_modules, input_module_params, filename_start: int):\n        result, execution_times = zip(*map(lambda task: measure_speed(\n            task[0], project_dir=project_dir, previous_result=previous_result, **task[1]),\n                                           zip(input_modules, input_module_params)))\n        average_times = list(map(lambda x: x / len(result[0]), execution_times))\n\n        # run metrics before filtering\n        if strategies.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for retrieval evaluation.\")\n        result = list(map(lambda x: evaluate_retrieval_node(x, retrieval_gt, strategies.get('metrics')), result))\n\n        # save results to folder\n        filepaths = list(map(lambda x: os.path.join(save_dir, f'{x}.parquet'),\n                             range(filename_start, filename_start + len(input_modules))))\n        list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(result, filepaths)))  # execute save to parquet\n        filename_list = list(map(lambda x: os.path.basename(x), filepaths))\n\n        summary_df = pd.DataFrame({\n            'filename': filename_list,\n            'module_name': list(map(lambda module: module.__name__, input_modules)),\n            'module_params': input_module_params,\n            'execution_time': average_times,\n            **{metric: list(map(lambda result: result[metric].mean(), result)) for metric in\n               strategies.get('metrics')},\n        })\n        summary_df.to_csv(os.path.join(save_dir, 'summary.csv'), index=False)\n        return result, average_times, summary_df\n\n    # run retrieval modules except hybrid\n    hybrid_module_names = ['hybrid_rrf', 'hybrid_cc']\n    filename_first = 0\n    if any([module.__name__ not in hybrid_module_names for module in modules]):\n        non_hybrid_modules, non_hybrid_module_params = zip(*filter(lambda x: x[0].__name__ not in hybrid_module_names,\n                                                                   zip(modules, module_params)))\n        non_hybrid_results, non_hybrid_times, non_hybrid_summary_df = run_and_save(non_hybrid_modules,\n                                                                                   non_hybrid_module_params, filename_first)\n        filename_first += len(non_hybrid_modules)\n    else:\n        non_hybrid_results, non_hybrid_times, non_hybrid_summary_df = [], [], pd.DataFrame()\n\n    if any([module.__name__ in hybrid_module_names for module in modules]):\n        hybrid_modules, hybrid_module_params = zip(*filter(lambda x: x[0].__name__ in hybrid_module_names,\n                                                           zip(modules, module_params)))\n        if all(['target_module_params' in x for x in hybrid_module_params]):\n            # If target_module_params are already given, run hybrid retrieval directly\n            hybrid_results, hybrid_times, hybrid_summary_df = run_and_save(hybrid_modules, hybrid_module_params,\n                                                                           filename_first)\n            filename_first += len(hybrid_modules)\n        else:\n            target_modules = list(map(lambda x: x.pop('target_modules'), hybrid_module_params))\n            target_filenames = list(map(lambda x: select_result_for_hybrid(save_dir, x), target_modules))\n            ids_scores = list(map(lambda x: get_ids_and_scores(save_dir, x), target_filenames))\n            target_module_params = list(map(lambda x: get_module_params(save_dir, x), target_filenames))\n            hybrid_module_params = list(map(lambda x: {**x[0], **x[1]}, zip(hybrid_module_params, ids_scores)))\n            real_hybrid_times = list(map(lambda filename: get_hybrid_execution_times(save_dir, filename), target_filenames))\n            hybrid_results, hybrid_times, hybrid_summary_df = run_and_save(hybrid_modules, hybrid_module_params,\n                                                                           filename_first)\n            filename_first += len(hybrid_modules)\n            hybrid_times = real_hybrid_times.copy()\n            hybrid_summary_df['execution_time'] = hybrid_times\n            hybrid_summary_df = edit_summary_df_params(hybrid_summary_df, target_modules, target_module_params)\n    else:\n        hybrid_results, hybrid_times, hybrid_summary_df = [], [], pd.DataFrame()\n\n    summary = pd.concat([non_hybrid_summary_df, hybrid_summary_df], ignore_index=True)\n    results = non_hybrid_results + hybrid_results\n    average_times = non_hybrid_times + hybrid_times\n    filenames = summary['filename'].tolist()\n\n    # filter by strategies\n    if strategies.get('speed_threshold') is not None:\n        results, filenames = filter_by_threshold(results, average_times, strategies['speed_threshold'], filenames)\n    selected_result, selected_filename = select_best_average(results, strategies.get('metrics'), filenames)\n    best_result = pd.concat([previous_result, selected_result], axis=1)\n\n    # add summary.csv 'is_best' column\n    summary['is_best'] = summary['filename'] == selected_filename\n\n    # save the result files\n    best_result.to_parquet(os.path.join(save_dir, f'best_{os.path.splitext(selected_filename)[0]}.parquet'),\n                           index=False)\n    summary.to_csv(os.path.join(save_dir, 'summary.csv'), index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run query expansion\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    pseudo_module_params = deepcopy(module_params)\n    for i, module_param in enumerate(pseudo_module_params):\n        if 'prompt' in module_params:\n            module_param['prompt'] = str(i)\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # check metrics in strategy\n        if general_strategy.get('metrics') is None:\n            raise ValueError(\"You must at least one metrics for query expansion evaluation.\")\n\n        if extra_strategy.get('top_k') is None:\n            extra_strategy['top_k'] = 10  # default value\n\n        # get retrieval modules from strategy\n        retrieval_callables, retrieval_params = make_retrieval_callable_params(extra_strategy)\n\n        # get retrieval_gt\n        retrieval_gt = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))['retrieval_gt'].tolist()\n\n        # run evaluation\n        evaluation_results = list(map(lambda result: evaluate_one_query_expansion_node(\n            retrieval_callables, retrieval_params, result['queries'].tolist(), retrieval_gt,\n            general_strategy['metrics'], project_dir, previous_result), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'query_expansion_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in general_strategy['metrics']}\n        })\n        summary_df = pd.merge(on='filename', left=summary_df, right=evaluation_df, how='left')\n\n        best_result, best_filename = select_best_average(evaluation_results, general_strategy['metrics'], filenames)\n        # change metric name columns to query_expansion_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'query_expansion_{metric_name}' for metric_name in strategies['metrics']})\n        best_result = best_result.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n        best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    project_dir = pathlib.PurePath(node_line_dir).parent.parent\n\n    # run modules\n    results, execution_times = zip(*map(lambda task: measure_speed(\n        task[0], project_dir=project_dir, previous_result=previous_result, **task[1]), zip(modules, module_params)))\n    average_times = list(map(lambda x: x / len(results[0]), execution_times))\n\n    # save results to folder\n    filepaths = list(map(lambda x: os.path.join(node_dir, f'{x}.parquet'), range(len(modules))))\n    list(map(lambda x: x[0].to_parquet(x[1], index=False), zip(results, filepaths)))  # execute save to parquet\n    filenames = list(map(lambda x: os.path.basename(x), filepaths))\n\n    # make summary file\n    summary_df = pd.DataFrame({\n        'filename': filenames,\n        'module_name': list(map(lambda module: module.__name__, modules)),\n        'module_params': module_params,\n        'execution_time': average_times,\n    })\n\n    metric_names, metric_params = cast_metrics(strategies.get('metrics'))\n\n    # Run evaluation when there are more than one module.\n    if len(modules) > 1:\n        # pop general keys from strategies (e.g. metrics, speed_threshold)\n        general_key = ['metrics', 'speed_threshold']\n        general_strategy = dict(filter(lambda x: x[0] in general_key, strategies.items()))\n        extra_strategy = dict(filter(lambda x: x[0] not in general_key, strategies.items()))\n\n        # first, filter by threshold if it is enabled.\n        if general_strategy.get('speed_threshold') is not None:\n            results, filenames = filter_by_threshold(results, average_times, general_strategy['speed_threshold'],\n                                                     filenames)\n\n        # run metrics before filtering\n        if metric_names is None or len(metric_names) <= 0:\n            raise ValueError(\"You must at least one metrics for prompt maker evaluation.\")\n\n        # get generator modules from strategy\n        generator_callables, generator_params = make_generator_callable_params(extra_strategy)\n\n        # get generation_gt\n        qa_data = pd.read_parquet(os.path.join(project_dir, \"data\", \"qa.parquet\"))\n        validate_qa_dataset(qa_data)\n        generation_gt = qa_data['generation_gt'].tolist()\n        generation_gt = list(map(lambda x: x.tolist(), generation_gt))\n\n        # run evaluations\n        evaluation_results = list(map(lambda result: evaluate_one_prompt_maker_node(\n            generator_callables, generator_params, result['prompts'].tolist(),\n            generation_gt, general_strategy['metrics'], project_dir), results))\n\n        evaluation_df = pd.DataFrame({\n            'filename': filenames,\n            **{f'prompt_maker_{metric_name}': list(map(lambda x: x[metric_name].mean(), evaluation_results))\n               for metric_name in metric_names}\n        })\n        summary_df = pd.merge(on='filename', left=summary_df, right=evaluation_df, how='left')\n\n        best_result, best_filename = select_best_average(evaluation_results, metric_names, filenames)\n        # change metric name columns to prompt_maker_metric_name\n        best_result = best_result.rename(columns={\n            metric_name: f'prompt_maker_{metric_name}' for metric_name in metric_names})\n        best_result = best_result.drop(columns=['generated_texts'])\n    else:\n        best_result, best_filename = results[0], filenames[0]\n\n    # add 'is_best' column at summary file\n    summary_df['is_best'] = summary_df['filename'] == best_filename\n\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # save files\n    summary_df.to_csv(os.path.join(node_dir, \"summary.csv\"), index=False)\n    best_result.to_parquet(os.path.join(node_dir, f\"best_{os.path.splitext(best_filename)[0]}.parquet\"), index=False)\n\n    return best_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models['huggingface_all_mpnet_base_v2']\n\n    gt_embeddings = embedding_model.get_text_embedding_batch(generation_gt)\n    pred_embedding = embedding_model.get_text_embedding(pred)\n\n    # calculate cosine similarity\n    similarity_scores: List[float] = list(map(lambda x: calculate_cosine_similarity(x, pred_embedding), gt_embeddings))\n    return max(similarity_scores)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer:\n        return gfpgan_face_restorer.restore(np_image)\n    logger.warning(\"GFPGAN face restorer not set up\")\n    return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        shared.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(\"Error setting up CodeFormer\", exc_info=True)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        shared.face_restorers.append(gfpgan_face_restorer)\n    except Exception:\n        errors.report(\"Error setting up GFPGAN\", exc_info=True)"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return im(multiply(multiply(q, q_v), conjugate(q)))"}
{"namespace": "quaternion.from_axis_angle", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  half_theta = theta / 2.0\n  k = jnp.sin(half_theta) / theta\n  # Avoid evaluating sqrt when theta is close to zero.\n  k = jnp.where(theta_squared > eps**2, k, 0.5)\n  qw = jnp.where(theta_squared > eps**2, jnp.cos(half_theta), 1.0)\n  qx = axis_angle[0] * k\n  qy = axis_angle[1] * k\n  qz = axis_angle[2] * k\n\n  return jnp.squeeze(jnp.array([qx, qy, qz, qw]))"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk_words = model.topk(prefix)\n    highest_idx = list(topk_words.keys())[np.argmax(list(topk_words.values()))]\n    if idx == highest_idx:\n        return topk_words[idx], k\n    num_calls = k\n\n    # initialize high\n    logit_bias = {idx: high}\n    new_max_idx = model.argmax(prefix, logit_bias)\n    num_calls += k\n    while new_max_idx != idx:\n        logit_bias[idx] *= 2\n        new_max_idx = model.argmax(prefix, logit_bias)\n        num_calls += k\n    high = logit_bias[idx]\n\n    output = model.topk(prefix, logit_bias)\n    num_calls += k\n\n    # compute normalizing constant\n    diff = topk_words[highest_idx] - output[highest_idx]\n    logZ = high - math.log(math.exp(diff) - 1)\n    fv = output[idx] + math.log(math.exp(logZ) + math.exp(high)) - high\n    logprob = fv - logZ\n\n    return logprob, num_calls"}
{"namespace": "resample.resample_3d", "completion": "  assert len(data.shape) >= 3\n  assert edge_behavior in ['CONSTANT_OUTSIDE', 'CLAMP']\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(\n        data,\n        np.array([[1, 1], [1, 1], [1, 1]] + (data.ndim - 3) * [[0, 0]]),\n        constant_values=constant_values,\n    )\n    locations = locations + 1.0\n\n  if method == 'TRILINEAR':\n    # Trilinearly interpolates by finding the weighted sum of the eight corner\n    # points.\n    if half_pixel_center:\n      locations = locations - 0.5\n    floored = jnp.floor(locations)\n    ceil = floored + 1.0\n    positions = [\n        jnp.stack([floored[Ellipsis, 0], floored[Ellipsis, 1], floored[Ellipsis, 2]], axis=-1),\n        jnp.stack([floored[Ellipsis, 0], floored[Ellipsis, 1], ceil[Ellipsis, 2]], axis=-1),\n        jnp.stack([floored[Ellipsis, 0], ceil[Ellipsis, 1], floored[Ellipsis, 2]], axis=-1),\n        jnp.stack([floored[Ellipsis, 0], ceil[Ellipsis, 1], ceil[Ellipsis, 2]], axis=-1),\n        jnp.stack([ceil[Ellipsis, 0], floored[Ellipsis, 1], floored[Ellipsis, 2]], axis=-1),\n        jnp.stack([ceil[Ellipsis, 0], floored[Ellipsis, 1], ceil[Ellipsis, 2]], axis=-1),\n        jnp.stack([ceil[Ellipsis, 0], ceil[Ellipsis, 1], floored[Ellipsis, 2]], axis=-1),\n        jnp.stack([ceil[Ellipsis, 0], ceil[Ellipsis, 1], ceil[Ellipsis, 2]], axis=-1),\n    ]\n    ceil_w = locations - floored\n    floor_w = 1.0 - ceil_w\n    weights = [\n        floor_w[Ellipsis, 0] * floor_w[Ellipsis, 1] * floor_w[Ellipsis, 2],\n        floor_w[Ellipsis, 0] * floor_w[Ellipsis, 1] * ceil_w[Ellipsis, 2],\n        floor_w[Ellipsis, 0] * ceil_w[Ellipsis, 1] * floor_w[Ellipsis, 2],\n        floor_w[Ellipsis, 0] * ceil_w[Ellipsis, 1] * ceil_w[Ellipsis, 2],\n        ceil_w[Ellipsis, 0] * floor_w[Ellipsis, 1] * floor_w[Ellipsis, 2],\n        ceil_w[Ellipsis, 0] * floor_w[Ellipsis, 1] * ceil_w[Ellipsis, 2],\n        ceil_w[Ellipsis, 0] * ceil_w[Ellipsis, 1] * floor_w[Ellipsis, 2],\n        ceil_w[Ellipsis, 0] * ceil_w[Ellipsis, 1] * ceil_w[Ellipsis, 2],\n    ]\n  elif method == 'NEAREST':\n    # Interpolate into the nearest cell. A weight of `None` is treated as 1.\n    positions = [(jnp.floor if half_pixel_center else jnp.round)(locations)]\n    weights = [None]\n  else:\n    raise ValueError('interpolation method {method} not supported')\n\n  max_indices = jnp.array(data.shape[:3], dtype=jnp.int32) - 1\n  if coordinate_order == 'xyz':\n    max_indices = jnp.flip(max_indices)\n\n  output = jnp.zeros((*locations.shape[:-1], data.shape[-1]), dtype=data.dtype)\n\n  for position, weight in zip(positions, weights):\n    indexes = position.astype(jnp.int32)\n\n    indexes = jnp.maximum(indexes, 0)\n    indexes = jnp.minimum(indexes, max_indices)\n    gathered = gather_volume(data, indexes, coordinate_order)\n    weighted_gathered = (\n        gathered if weight is None else gathered * weight[Ellipsis, None]\n    )\n    output += weighted_gathered\n\n  return output.astype(data.dtype)"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == -jnp.inf, 1),\n          (p >= 0, jnp.inf),\n      ],\n      safe_div(p - 1, p),\n  )"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    verts = np.array([\n        (np.sqrt(8 / 9), 0, -1 / 3),\n        (-np.sqrt(2 / 9), np.sqrt(2 / 3), -1 / 3),\n        (-np.sqrt(2 / 9), -np.sqrt(2 / 3), -1 / 3),\n        (0, 0, 1),\n    ])\n    faces = np.array([(0, 1, 2), (0, 2, 3), (0, 1, 3), (1, 2, 3)])\n  elif base_shape == 'icosahedron':\n    a = (np.sqrt(5) + 1) / 2\n    verts = np.array([\n        (-1, 0, a),\n        (1, 0, a),\n        (-1, 0, -a),\n        (1, 0, -a),\n        (0, a, 1),\n        (0, a, -1),\n        (0, -a, 1),\n        (0, -a, -1),\n        (a, 1, 0),\n        (-a, 1, 0),\n        (a, -1, 0),\n        (-a, -1, 0),\n    ]) / np.sqrt(a + 2)\n    faces = np.array([\n        (0, 4, 1),\n        (0, 9, 4),\n        (9, 5, 4),\n        (4, 5, 8),\n        (4, 8, 1),\n        (8, 10, 1),\n        (8, 3, 10),\n        (5, 3, 8),\n        (5, 2, 3),\n        (2, 7, 3),\n        (7, 10, 3),\n        (7, 6, 10),\n        (7, 11, 6),\n        (11, 0, 6),\n        (0, 1, 6),\n        (6, 1, 10),\n        (9, 0, 11),\n        (9, 11, 2),\n        (9, 2, 5),\n        (7, 2, 11),\n    ])\n  elif base_shape == 'octahedron':\n    verts = np.array(\n        [(0, 0, -1), (0, 0, 1), (0, -1, 0), (0, 1, 0), (-1, 0, 0), (1, 0, 0)]\n    )\n    corners = np.array(list(itertools.product([-1, 1], repeat=3)))\n    pairs = np.argwhere(compute_sq_dist(corners.T, verts.T) == 2)\n    faces = np.sort(np.reshape(pairs[:, 1], [3, -1]).T, 1)\n  else:\n    raise ValueError(f'base_shape {base_shape} not supported')\n  verts = tesselate_geodesic(verts, faces, angular_tesselation)\n\n  if remove_symmetries:\n    # Remove elements of `verts` that are reflections of each other.\n    match = compute_sq_dist(verts.T, -verts.T) < eps\n    verts = verts[~np.any(np.triu(match), axis=0), :]\n\n  basis = verts[:, ::-1]\n  return basis"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n  xp = jnp.abs(x)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y /= postmult\n  yp = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if premult is not None:\n    x /= premult\n  return x"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      random.PRNGKey(0),\n      n=100,\n      origin_lo=-1.5,\n      origin_hi=1.5,\n      radius_lo=1e-5,\n      radius_hi=1e-3,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=10,\n      far_hi=10000,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(f'points_to_pixels only supports perspective projection, '\n                     f'not {camtype} mode.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  rotation = camtoworlds[Ellipsis, :3, :3]\n  rotation_inv = xnp.swapaxes(rotation, -1, -2)\n  translation = camtoworlds[Ellipsis, :3, -1]\n  # Points (directions) in the camera coordinate frame.\n  points_camera = mat_vec_mul(rotation_inv, points - translation)\n\n  # Projection to image plane by dividing out -z.\n  depth = -points_camera[Ellipsis, -1]\n  camera_dirs = points_camera / depth[Ellipsis, None]\n\n  # OpenGL to OpenCV coordinates.\n  camera_dirs = matmul(camera_dirs, xnp.diag(xnp.array([1.0, -1.0, -1.0])))\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        camera_dirs[Ellipsis, 0],\n        camera_dirs[Ellipsis, 1],\n        **distortion_params,\n    )\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Apply intrinsics matrix.\n  pixel_dirs = mat_vec_mul(xnp.linalg.inv(pixtocams), camera_dirs)\n\n  # Remove half pixel offset.\n  coordinates = pixel_dirs[Ellipsis, :2] - xnp.array([0.5, 0.5])\n\n  return coordinates, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = jnp.split(screw_axis, 2)\n  R = exp_so3(w)\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(w / theta)\n  # Note that p = 0 when theta = 0.\n  p = spin_math.matmul(\n      (\n          theta * jnp.eye(3)\n          + (1.0 - jnp.cos(theta)) * W\n          + (theta - jnp.sin(theta)) * spin_math.matmul(W, W)\n      ),\n      v / theta,\n  )\n  # If theta^2 is close to 0 it means this is a pure translation so p = v.\n  p = jnp.where(theta_squared > eps**2, p, v)\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(axis_angle)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  axis_angle_safe = jnp.where(theta_squared > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  axis = axis_angle_safe / theta_safe\n  W = skew(axis)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n\n  return jnp.where(theta_squared > eps**2, R, R_taylor)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0) ** 2 / 12\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_dirs_stacked = xnp.stack(\n        [\n            camera_dirs_stacked[Ellipsis, 0] * sin_theta_over_theta,\n            camera_dirs_stacked[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = camera_dirs_stacked[Ellipsis, 0]\n    phi = camera_dirs_stacked[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    camera_dirs_stacked = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  camera_dirs_stacked = matmul(\n      camera_dirs_stacked, xnp.diag(xnp.array([1.0, -1.0, -1.0]))\n  )\n\n  # Extract 2D image plane (x, y) coordinates.\n  imageplane = camera_dirs_stacked[0, Ellipsis, :2]\n\n  # Apply camera rotation matrices.\n  directions_stacked = mat_vec_mul(\n      camtoworlds[Ellipsis, :3, :3], camera_dirs_stacked\n  )\n  # Extract the offset rays.\n  directions, dx, dy = directions_stacked\n\n  origins = xnp.broadcast_to(camtoworlds[Ellipsis, :3, -1], directions.shape)\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  if pixtocam_ndc is None:\n    # Distance from each unit-norm direction vector to its neighbors.\n    dx_norm = xnp.linalg.norm(dx - directions, axis=-1)\n    dy_norm = xnp.linalg.norm(dy - directions, axis=-1)\n\n  else:\n    # Convert ray origins and directions into projective NDC space.\n    ndc_fn = functools.partial(convert_to_ndc, pixtocam=pixtocam_ndc, xnp=xnp)\n    origins_dx, _ = ndc_fn(origins, dx)\n    origins_dy, _ = ndc_fn(origins, dy)\n    origins, directions = ndc_fn(origins, directions)\n\n    # In NDC space, we use the offset between origins instead of directions.\n    dx_norm = xnp.linalg.norm(origins_dx - origins, axis=-1)\n    dy_norm = xnp.linalg.norm(origins_dy - origins, axis=-1)\n\n  # Cut the distance in half, multiply it to match the variance of a uniform\n  # distribution the size of a pixel (1/12, see paper).\n  # TODO(barron): Add a unit test that this is correct.\n  radii = (0.5 * (dx_norm + dy_norm))[Ellipsis, None] * 2 / xnp.sqrt(12)\n\n  return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "render.compute_alpha_weights", "completion": "  t_delta = jnp.diff(tdist)\n  delta = t_delta * jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)\n  density_delta = density * delta\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    if deterministic_center:\n      pad = 1 / (2 * num_samples)\n      u = jnp.linspace(pad, 1.0 - pad - eps, num_samples)\n    else:\n      u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  return invert_cdf(u, t, w_logits)"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Sample a set of points from the step function.\n  centers = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # The intervals we return will span the midpoints of each adjacent sample.\n  mid = (centers[Ellipsis, 1:] + centers[Ellipsis, :-1]) / 2\n\n  # Each first/last fencepost is the reflection of the first/last midpoint\n  # around the first/last sampled center.\n  first = 2 * centers[Ellipsis, :1] - mid[Ellipsis, :1]\n  last = 2 * centers[Ellipsis, -1:] - mid[Ellipsis, -1:]\n  samples = jnp.concatenate([first, mid, last], axis=-1)\n\n  # We clamp to the limits of the input domain, provided by the caller.\n  samples = jnp.clip(samples, *domain)\n  return samples"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  cw = integrate_weights(w)\n  # We want to interpolate into the integrated weights according to `ps`.\n  wprctile = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      jnp.array(ps) / 100, cw, t\n  )\n  return wprctile"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Convert the histogram to a PDF.\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF step function into a piecewise linear spline PDF.\n  t_linspline, p_linspline = linspline.blur_stepfun(t, p, blur_halfwidth)\n\n  # Integrate the spline PDF, then query it to get integrated weights.\n  quad = linspline.compute_integral(t_linspline, p_linspline)\n  acc_wq = linspline.interpolate_integral(tq, t_linspline, *quad)\n\n  # Undo the integration to get weights.\n  wq = jnp.diff(acc_wq, axis=-1)\n\n  # Fix negative values to 0, as they should never happen but may due to\n  # numerical issues.\n  wq = jnp.maximum(0, wq)\n  return wq"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_h = to_homogeneous(vectors.reshape((-1, vectors.shape[-1])))\n  transformed = from_homogeneous(matmul(transform, vectors_h.T).T)\n  return transformed.reshape(vectors.shape)"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    wp = jnp.diff(tp)\n    v_numer = resample(t, tp, vp * wp, use_avg=False)\n    v_denom = resample(t, tp, wp, use_avg=False)\n    v = math.safe_div(v_numer, v_denom)\n    return v\n\n  acc = jnp.cumsum(vp, axis=-1)\n  acc0 = jnp.concatenate([jnp.zeros(acc.shape[:-1] + (1,)), acc], axis=-1)\n  acc0_resampled = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      t, tp, acc0\n  )\n  v = jnp.diff(acc0_resampled, axis=-1)\n  return v"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = jnp.reshape(mean[Ellipsis, None, :] * scales[:, None], shape)\n  scaled_var = jnp.reshape(var[Ellipsis, None, :] * scales[:, None] ** 2, shape)\n\n  return expected_sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1),\n      jnp.concatenate([scaled_var] * 2, axis=-1),\n  )"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding (DE).\"\"\"\n    return integrated_dir_enc_fn(xyz, jnp.zeros_like(xyz[Ellipsis, :1]))\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    running_line = \"\"\n    line_buffer = []\n    line_type = \"para\"\n    header_block_idx = -1\n    block_idx = 0\n    line_set = set()\n    for line_str in lines:\n        # print(line_str)\n        line_str = clean_line(line_str)\n\n        if should_skip(line_str, xml=xml):\n            continue\n        line_without_numbers = re.sub(r\"\\d+\", \"\", line_str)\n        if line_without_numbers in line_set:\n            continue\n        else:\n            line_set.add(line_without_numbers)\n\n        curr_line = line_parser.Line(line_str)\n\n        # this converst strings like 'e x e c u t i v e summary' to 'executive summary'\n        if not xml and curr_line.has_spaced_characters:\n            line_str = fix_spaced_characters(line_str)\n            curr_line = line_parser.Line(line_str)\n\n        if len(line_buffer) > 0:\n\n            # find out if previous line was a discontinous line\n            prev_line = line_buffer[-1]\n\n            logger.debug(\"========\")\n            logger.debug(f\"{prev_line.incomplete_line} >> {prev_line.text} \\n\")\n            logger.debug(f\"{curr_line.continuing_line} >> {curr_line.text} \\n\")\n            # keep connecting lines as long as they seem incomplete\n            is_incomplete = prev_line.incomplete_line or (\n                len(line_buffer) > 1 and not prev_line.ends_with_period\n            )\n            logger.debug(\n                f\"incomplete: {is_incomplete}, is_list_or_row: {curr_line.is_list_or_row}, continuing_line: {curr_line.continuing_line}\",\n            )\n            if (\n                is_incomplete\n                and not (curr_line.is_list_or_row or curr_line.line_type == \"list_item\")\n            ) or curr_line.continuing_line:\n                logger.debug(\"connecting..\")\n                running_line = formatter.connect(running_line, curr_line.text)\n                line_buffer.append(curr_line)\n                # if we are connecting lines, then this has to be a para unless it is a list_item, basically no headers\n                if not line_type == \"list_item\":\n                    line_type = \"para\"\n            else:  # commit the line and start a new line\n                # remove different types of bulletted list (for better formatting) but do not touch numbered line\n                logger.debug(\"starting new line..\")\n                # if line_type == \"list_item\":\n                #     running_line = running_line[1:].lstrip()\n\n                if line_type == \"header\":\n                    header_block_idx = block_idx\n\n                block = {\n                    \"block_idx\": block_idx,\n                    \"block_text\": running_line,\n                    \"block_type\": line_type,\n                    \"text_group_start_idx\": -1,\n                    \"block_list\": [],\n                    \"header_block_idx\": header_block_idx,\n                    \"level\": 0,\n                }\n\n                result.append(block)\n\n                block_idx = block_idx + 1\n\n                running_line = curr_line.text\n                line_buffer = [curr_line]\n                line_type = curr_line.line_type\n            logger.debug(\"========\")\n        else:\n            running_line = curr_line.text\n            line_type = curr_line.line_type\n            line_buffer = [curr_line]\n\n    if line_type == \"list_item\" and running_line[0] in \"\ufffd\\\\*,.?\u2022\\\\\u27a2\u0192\uf0b7\u2013\\\\'\\\"\u2014\":\n        running_line = running_line[1:].lstrip()\n\n    block = {\n        \"block_idx\": block_idx,\n        \"block_text\": running_line,\n        \"block_type\": line_type,\n        \"text_group_start_idx\": -1,\n        \"block_list\": [],\n        \"header_block_idx\": header_block_idx,\n        \"level\": 0,\n    }\n\n    result.append(block)\n    return result"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    sents = []\n\n    # in case org_texts has \\n, break it into multiple paragraph\n    # edge case for html and markdown\n    for org_text in org_texts.split(\"\\n\"):\n        org_text = space_rule.sub(r'\\1', org_text)\n        modified_text = re.sub(r'^([.,?!]\\s+)+', \"\", org_text)  # To handle bug https://github.com/nltk/nltk/issues/2925\n        orig_offset = abs(len(org_text) - len(modified_text))\n\n        # do not break bracket\n        for span_group in bracket_rule.finditer(modified_text):\n            start_byte, end_byte = span_group.span()\n            span = modified_text[start_byte:end_byte]\n            # skip this logic when span is too big? disabled for now\n            # if len(span.split()) >= 10:\n            #     continue\n            modified_text = modified_text.replace(\n                f\"({span})\", f\"_{span.replace('.','_')}_\",\n            )\n\n        for rule, replaced in rules:\n            modified_text = rule.sub(replaced, modified_text)\n        # Normalize all the quotation.\n        modified_text = quotation_pattern.sub(\"\\\"\", modified_text)\n\n        modified_sents = nltk_tokenzier.tokenize(modified_text)\n\n        offset = orig_offset\n        sent_idx = 0\n        while offset < len(modified_text) and sent_idx < len(modified_sents):\n            if modified_text[offset] == \" \":\n                offset += 1\n                continue\n\n            # cut org_text based on lengths of modified_sent\n            modified_sent = modified_sents[sent_idx]\n            sents.append(org_text[offset: offset + len(modified_sent)])\n\n            offset += len(modified_sent)\n            sent_idx += 1\n    if len(sents) >= 2 and re.match(r\"^.\\.$\", sents[0]):\n        sents[1] = sents[0] + \" \" + sents[1]\n        sents = sents[1:]\n\n    return sents"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        term_id = self.term_dict.get_term_id(token)\n        key = self.term_mat.rows[key] if key is not None else self.term_mat.rows\n        posns = self.posns.positions(term_id, doc_ids=key)\n        return posns"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    def checked_parse_int(value, error_message):\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(error_message)\n\n    result = num_clauses\n    spec = spec.strip()\n\n    if '<' in spec:\n        # we have conditional spec(s)\n        space_around_less_than_pattern = re.compile(r'\\s*<\\s*')\n        spec = space_around_less_than_pattern.sub('<', spec)\n        for s in spec.split():\n            parts = s.split('<', 1)\n            if len(parts) < 2:\n                raise ValueError(\"Invalid 'mm' spec: '\" + s + \"'. Expecting values before and after '<'\")\n            upper_bound = checked_parse_int(parts[0], \"Invalid 'mm' spec. Expecting an integer.\")\n            if num_clauses <= upper_bound:\n                return result\n            else:\n                result = parse_min_should_match(num_clauses, parts[1])\n        return result\n\n    # otherwise, simple expression\n    if '%' in spec:\n        # percentage - assume the % was the last char. If not, let int() fail.\n        spec = spec[:-1]\n        percent = checked_parse_int(spec, \"Invalid 'mm' spec. Expecting an integer.\")\n        calc = (result * percent) * (1 / 100)\n        result = result + int(calc) if calc < 0 else int(calc)\n    else:\n        calc = checked_parse_int(spec, \"Invalid 'mm' spec. Expecting an integer.\")\n        result = result + calc if calc < 0 else calc\n\n    return min(num_clauses, max(result, 0))"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            phrase_freqs = np.zeros(len(self))\n            try:\n                doc_ids = self.term_mat.rows\n                term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n                return self.posns.phrase_freqs(term_ids, doc_ids=doc_ids,\n                                               phrase_freqs=phrase_freqs)\n            except TermMissingError:\n                return phrase_freqs\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens =\\\n            build_index_from_tokenizer(array, tokenizer, batch_size=batch_size,\n                                       truncate=truncate)\n\n        postings = cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n        postings.term_mat = term_mat\n        postings.posns = posns\n        postings.term_dict = term_dict\n        postings.avg_doc_length = avg_doc_length\n        postings.doc_lens = doc_lens\n        return postings"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            server_host=self.config['serverHost'],\n            server_port=self.config['serverPort'],\n            proxy_host=self.config['proxyHost'],\n            proxy_port=self.config['proxyPort'],\n            strategy=ProxifierStrategyType(self.config['strategy']),\n            strategy_config=self.config['strategies'].get(self.config['strategy'], {}),\n            logger=self.logger\n        )\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n\n    arr += (arr >> _4)\n    arr &= s0F\n    arr *= s01\n    arr >>= all_but_one_bit\n\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    def listify(x):\n        return x if isinstance(x, list) else [x]\n\n    query_fields = parse_field_boosts(listify(qf))\n    phrase_fields = parse_field_boosts(listify(pf)) if pf else {}\n    if mm is None:\n        mm = \"1\"\n    if q_op == \"AND\":\n        mm = \"100%\"\n\n    # bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    # trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields,\n                                                   num_search_terms, search_terms, mm,\n                                                   similarity=similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields,\n                                                    num_search_terms, search_terms, mm,\n                                                    similarity=similarity)\n\n    phrase_scores = []\n    for field, boost in phrase_fields.items():\n        arr = get_field(frame, field)\n        terms = search_terms[field]\n        field_phrase_score = arr.score(terms, similarity=similarity) * (1 if boost is None else boost)\n        boost_exp = f\"{boost}\" if boost is not None else \"1\"\n        explain += f\" ({field}:\\\"{' '.join(terms)}\\\")^{boost_exp}\"\n        phrase_scores.append(field_phrase_score)\n\n    if len(phrase_scores) > 0:\n        phrase_scores = np.sum(phrase_scores, axis=0)\n        # Add where term_scores > 0\n        term_match_idx = np.where(qf_scores)[0]\n\n        qf_scores[term_match_idx] += phrase_scores[term_match_idx]\n    return qf_scores, explain"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = self._get_connection(message).c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = self._get_connection(message).s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        if hasattr(self, 'connections'):\n            for connection in self.connections.values():\n                connection.stop()\n        if hasattr(self, 'server'):\n            self.server.stop()"}
